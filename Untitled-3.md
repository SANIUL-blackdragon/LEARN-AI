#### **Unit 5: Computer Systems & Hardware Basics**  

# Topic 1: Introduction to Computer Systems

Students will be assessed on their ability to:

**1.1.1 Define the components of a computer system**
- Identify hardware as the physical parts of a computer system
- Identify software as the programs and operating systems that run on hardware
- Explain the role of data in a computer system
- Describe how users interact with computer systems

**Guidance:** Students should understand that a computer system consists of four main components: hardware (physical components like CPU, memory, input/output devices), software (programs including operating systems and applications), data (information processed by the system), and users (people who interact with the system). They should be able to give examples of each component and explain that all components must work together for the system to function. Examples should include common devices like smartphones (which contain hardware components, operating system software, apps, and user data) and how users interact with them through touch interfaces.

**1.1.2 Explain the input-process-output model**
- Define input as data entered into a computer system
- Define processing as the manipulation of data by the computer
- Define output as the results presented to the user
- Apply the input-process-output model to everyday computing tasks

**Guidance:** Students should understand the input-process-output model as a fundamental concept in computing. Input refers to data or instructions entered into the system (via keyboard, mouse, microphone, camera, etc.). Processing is what the computer does to transform the input (calculations, comparisons, sorting, etc.). Output is the result of processing (displayed on screen, printed, played through speakers, etc.). Students should be able to trace this model through common activities like searching the internet (input: search query, processing: search algorithm, output: search results) or playing a game (input: controller movements, processing: game logic, output: visual and audio feedback).

**1.1.3 Classify different types of computer systems**
- Identify personal computers (desktop, laptop, tablet)
- Identify mobile devices (smartphones, wearable technology)
- Identify servers and mainframes
- Identify embedded systems and specialized computers

**Guidance:** Students should be able to distinguish between different categories of computer systems based on their size, purpose, and capabilities. Personal computers include desktops (stationary, powerful), laptops (portable, integrated), and tablets (touch-based, highly portable). Mobile devices include smartphones (multipurpose, connected) and wearable technology (smartwatches, fitness trackers). Servers and mainframes are powerful systems designed to provide services to multiple users or handle large-scale processing. Embedded systems are specialized computers built into other devices (like in cars, appliances, or industrial equipment). Students should understand the key characteristics and typical uses of each type.

**1.1.4 Identify the role of computer systems in various contexts**
- Explain the use of computer systems in education
- Explain the use of computer systems in business
- Explain the use of computer systems in communication
- Explain the use of computer systems in entertainment

**Guidance:** Students should understand how computer systems are used in different sectors of society. In education, computers are used for research, online learning, educational software, and administration. In business, they handle data processing, communication, accounting, inventory management, and customer service. For communication, computers enable email, video conferencing, social media, and instant messaging. In entertainment, they power video games, streaming services, digital content creation, and virtual reality. Students should be able to provide specific examples of computer applications in each context and explain how they have transformed these sectors.

**1.1.5 Explain the von Neumann architecture**
- Describe the stored-program concept
- Identify the main components of von Neumann architecture
- Explain the fetch-decode-execute cycle
- Apply von Neumann principles to modern computers

**Guidance:** Students should understand the von Neumann architecture as the foundation of most modern computers. The key concept is the stored-program principle, where both program instructions and data are stored in the same memory. The main components include the central processing unit (CPU), memory, input/output systems, and the buses that connect them. Students should understand the basic fetch-decode-execute cycle: the CPU fetches an instruction from memory, decodes what the instruction means, and then executes it. They should recognize that while modern computers have many enhancements, they still follow these basic von Neumann principles. Examples should include how this architecture allows computers to be general-purpose machines that can run different programs.

**1.1.6 Identify components of a computer system**
- Recognize the central processing unit (CPU) and its function
- Recognize memory (RAM and ROM) and its purpose
- Identify storage devices and their characteristics
- Identify input and output devices and their functions

**Guidance:** Students should be able to identify and explain the function of key hardware components. The CPU is the "brain" of the computer that processes instructions and performs calculations. Memory includes RAM (temporary, volatile memory for active programs and data) and ROM (permanent, non-volatile memory for essential system instructions). Storage devices include hard disk drives (HDDs), solid-state drives (SSDs), USB drives, and optical discs, which retain data even when powered off. Input devices (keyboard, mouse, microphone, camera) allow users to enter data, while output devices (monitor, printer, speakers) present results to users. Students should understand the basic characteristics of each component type and how they contribute to the overall system.

**1.1.7 Understand data flow in a computer system**
- Trace the path of data through input devices
- Explain how data is processed by the CPU
- Describe how data is stored in memory and storage
- Trace the path of data to output devices

**Guidance:** Students should understand how data moves through a computer system. When data enters through input devices, it's typically converted to digital signals and sent to the CPU via the system bus. The CPU processes the data using temporary storage in registers and cache memory. Data may be stored temporarily in RAM for quick access or permanently in storage devices. When processing is complete, results are sent to output devices via the system bus. Students should be able to trace this flow for common tasks like saving a document (input from keyboard, processing by CPU, storage in memory and then to storage device, output to screen showing the saved document) or playing music (input from user interface, processing by CPU, retrieval from storage, output through speakers).

**1.1.8 Apply architectural concepts to simple scenarios**
- Analyze simple computing scenarios in terms of computer architecture
- Identify the components involved in basic computing tasks
- Explain how different components work together to complete tasks
- Predict the impact of changing one component on system performance

**Guidance:** Students should be able to apply their understanding of computer architecture to analyze real-world scenarios. For example, when using a word processor, they should identify that input comes from the keyboard, processing happens in the CPU, temporary storage occurs in RAM, and output appears on the screen. When playing a video game, they should recognize that input comes from controllers, processing involves complex calculations in the CPU and graphics processor, and output includes both visual and audio components. Students should be able to explain how components work together (e.g., how faster RAM can improve game performance by allowing quicker access to game data) and predict the effects of component changes (e.g., how adding more storage might allow more games to be installed but wouldn't necessarily make them run faster).

**1.1.9 Explain the interdependence of hardware and software**
- Describe how hardware relies on software to function
- Describe how software relies on hardware to execute
- Give examples of hardware-software compatibility issues
- Explain the concept of abstraction layers in computing

**Guidance:** Students should understand that hardware and software are interdependent - hardware is useless without software to direct it, and software cannot execute without appropriate hardware. They should learn that software provides instructions that hardware components follow, while hardware provides the physical means to execute those instructions. Examples of compatibility issues might include trying to run software designed for one operating system on another, or attempting to use software that requires more processing power than available. Students should be introduced to the concept of abstraction layers - how complex systems are built in layers, with each layer hiding the complexity of the layer below (e.g., applications don't need to know hardware details, they communicate through the operating system).

**1.1.10 Classify different types of software**
- Distinguish between system software and application software
- Identify different types of system software (operating systems, utilities, drivers)
- Identify different categories of application software
- Explain the purpose of different software types

**Guidance:** Students should be able to classify software into major categories. System software manages the computer hardware and provides a platform for applications. This includes operating systems (Windows, macOS, Linux, Android, iOS) that manage resources and provide user interfaces; utility software (antivirus, file management, backup tools) that helps maintain the system; and device drivers that allow hardware to communicate with the operating system. Application software performs specific tasks for users and includes categories like productivity software (word processors, spreadsheets), creative software (image editing, video production), communication software (email clients, web browsers), and entertainment software (games, media players). Students should understand the purpose of each type and be able to give examples.

**1.1.11 Understand how software instructions are executed**
- Explain the concept of machine code and high-level languages
- Describe the process of compilation and interpretation
- Understand how the CPU executes instructions
- Apply the concept of instruction execution to simple programs

**Guidance:** Students should understand that software is written in programming languages that humans can read, but must be translated into machine code (binary) that the CPU can execute. They should learn about two main translation methods: compilation (where the entire program is converted to machine code before execution) and interpretation (where instructions are translated and executed one at a time). Students should understand that the CPU executes instructions by fetching them from memory, decoding what operation to perform, performing the operation, and storing the result. They should be able to trace this process for simple programs, like calculating the sum of two numbers (the program adds instruction is fetched, decoded, executed by the ALU, and the result is stored).

**1.1.12 Apply hardware-software interaction concepts**
- Analyze how specific software utilizes hardware components
- Explain how hardware limitations affect software performance
- Identify how software can optimize hardware usage
- Apply hardware-software interaction concepts to real-world scenarios

**Guidance:** Students should be able to analyze how software makes use of hardware components. For example, video editing software uses the CPU for general processing, the GPU for rendering effects, RAM for storing video frames being edited, and storage for saving the final video. They should understand how hardware limitations affect software - like how insufficient RAM can cause a system to slow down when running multiple programs, or how an older CPU might struggle with newer software. Students should also learn how software can optimize hardware usage, such as through compression algorithms that reduce storage requirements, or caching strategies that make better use of available memory. They should apply these concepts to scenarios like choosing appropriate software for available hardware, or troubleshooting performance issues.

**1.1.13 Define computer performance metrics**
- Explain processing speed and how it's measured
- Explain memory capacity and how it's measured
- Explain storage capacity and how it's measured
- Explain responsiveness and how it's experienced by users

**Guidance:** Students should understand key metrics used to measure computer performance. Processing speed is typically measured in gigahertz (GHz) for CPUs, indicating how many instructions can be processed per second. Memory capacity (RAM) is measured in gigabytes (GB) and determines how much data can be quickly accessed by the CPU. Storage capacity is also measured in gigabytes or terabytes (TB) and indicates how much data can be permanently stored. Responsiveness refers to how quickly a computer reacts to user input and is experienced as lag or delay. Students should understand these metrics in practical terms - like how a 3.0 GHz processor is generally faster than a 2.0 GHz one, or how 8GB of RAM allows more programs to run simultaneously than 4GB. They should be able to relate these metrics to their own experiences with technology.

**1.1.14 Identify factors affecting computer performance**
- Recognize how CPU specifications affect performance
- Recognize how memory specifications affect performance
- Recognize how storage type and speed affect performance
- Recognize how software efficiency affects performance

**Guidance:** Students should understand the various factors that influence computer performance. CPU specifications that matter include clock speed (GHz), number of cores (more cores allow more simultaneous processing), and cache size (larger cache allows faster access to frequently used data). Memory factors include capacity (more RAM allows more programs to run at once) and speed (faster RAM allows quicker data access). Storage factors include type (SSDs are faster than HDDs), speed (measured in read/write speeds), and interface (SATA, NVMe). Software efficiency refers to how well programs are written - well-optimized software can perform the same tasks with fewer resources. Students should understand that performance is determined by the combination of these factors, not just one component, and that the slowest component (bottleneck) often limits overall performance.

**1.1.15 Compare performance of different computer systems**
- Compare different computer systems based on specifications
- Evaluate the suitability of systems for different tasks
- Explain why certain systems are better for specific applications
- Apply performance comparison concepts to real-world purchasing decisions

**Guidance:** Students should be able to compare computer systems based on their specifications and intended use. They should learn to evaluate which components are most important for different tasks - like how a powerful CPU and GPU are crucial for gaming and video editing, while ample RAM and fast storage are more important for multitasking and large data processing. Students should understand why certain systems are better suited for specific applications (e.g., why a workstation with a powerful CPU and lots of RAM is better for scientific computing than a basic laptop). They should apply these concepts to practical scenarios like choosing between different computer models for purchase, comparing specifications relative to needs and budget, and justifying their choices based on performance requirements for intended tasks.

**1.1.16 Apply performance concepts to real-world scenarios**
- Analyze performance requirements for different computing tasks
- Identify performance bottlenecks in computing scenarios
- Suggest upgrades or optimizations to improve performance
- Evaluate the cost-effectiveness of performance improvements

**Guidance:** Students should be able to apply their understanding of computer performance to real-world situations. They should analyze what different tasks require in terms of processing power, memory, storage, and responsiveness - like how video editing needs a powerful CPU and GPU, lots of RAM, and fast storage, while basic web browsing has modest requirements. Students should learn to identify performance bottlenecks (components that limit overall system performance) in scenarios like a slow-loading game (might be limited by GPU, CPU, RAM, or storage speed). They should suggest appropriate upgrades or optimizations, such as adding more RAM to a system that slows down when running multiple programs. Students should also consider cost-effectiveness - understanding that some upgrades offer significant performance improvements for their cost, while others provide minimal benefits. They should apply these concepts to scenarios like upgrading an existing system or choosing a new computer for specific needs.



# Topic 2: Central Processing Unit (CPU)

Students will be assessed on their ability to:

**2.2.1 Define the CPU and its role in computer systems**
- Define the Central Processing Unit (CPU) as the primary component responsible for executing instructions
- Explain the CPU's role as the "brain" of the computer
- Describe how the CPU controls and coordinates other computer components
- Understand the importance of the CPU in overall system performance

**Guidance:** Students should understand the CPU as the primary component in a computer that carries out the instructions of a computer program by performing basic arithmetic, logical, control, and input/output operations. They should learn that the CPU acts as the "brain" because it interprets and executes most of the commands from the computer's other hardware and software. Students should understand that the CPU controls and coordinates all other components by sending and receiving control signals, data, and instructions through the computer's bus system. They should recognize that the CPU's performance directly impacts the overall speed and capability of the computer system. Examples should include how a faster CPU can run more complex software, handle more tasks simultaneously, and provide better responsiveness in applications.

**2.2.2 Explain the historical development of CPUs**
- Identify key milestones in CPU development
- Explain Moore's Law and its impact on CPU evolution
- Describe how CPUs have evolved from early processors to modern multi-core chips
- Understand the relationship between technological advances and CPU capabilities

**Guidance:** Students should learn about the historical development of CPUs, starting from early processors like the Intel 4004 (1971) through to modern multi-core processors. They should understand Moore's Law (the observation that the number of transistors on a chip doubles approximately every two years) and how it has driven the exponential growth in computing power. Students should be able to describe the evolution from single-core processors to multi-core designs, and how advances in manufacturing processes (measured in nanometers) have allowed for smaller, more powerful, and more energy-efficient CPUs. They should understand how these technological advances have enabled CPUs to handle increasingly complex tasks, from simple calculations to artificial intelligence and real-time 3D graphics.

**2.2.3 Describe the physical characteristics and packaging of CPUs**
- Identify the physical appearance and size of CPUs
- Explain the materials used in CPU construction
- Describe the packaging and socket types used for CPUs
- Understand the importance of thermal management for CPUs

**Guidance:** Students should be able to describe the physical characteristics of CPUs, including their typically small square or rectangular shape (usually 1-2 inches per side) and their appearance as a flat, metal-covered chip with hundreds or thousands of connectors on the underside. They should learn about the materials used in CPU construction, primarily silicon for the semiconductor wafer, with metal (usually aluminum or copper) for connectors and heat dissipation. Students should understand different packaging types (like PGA - Pin Grid Array and LGA - Land Grid Array) and how these determine how CPUs connect to the motherboard through specific socket types. They should also understand the importance of thermal management, including heat sinks, fans, and thermal paste, in preventing CPUs from overheating due to the heat generated during operation.

**2.2.4 Explain how the CPU interacts with other computer components**
- Describe the pathways for communication between the CPU and other components
- Explain the role of the bus system in CPU communication
- Understand how the CPU accesses memory and storage
- Explain how the CPU communicates with input/output devices

**Guidance:** Students should understand how the CPU communicates with other computer components through various pathways. They should learn about the bus system, which consists of the data bus (transfers data), address bus (specifies memory locations), and control bus (carries control signals). Students should understand how the CPU uses the memory controller to access RAM for temporary data storage and how it communicates with storage devices through storage controllers (like SATA or NVMe controllers). They should learn how the CPU interacts with input/output devices through I/O controllers and interfaces (like USB, HDMI, or audio controllers). Students should be able to trace the flow of information between the CPU and other components during common operations, such as loading a program from storage into memory, processing it, and sending output to a display.

**2.2.5 Identify and explain the function of the Control Unit**
- Define the Control Unit as a component of the CPU
- Explain the role of the Control Unit in directing operations
- Describe how the Control Unit manages the fetch-decode-execute cycle
- Understand how the Control Unit coordinates other CPU components

**Guidance:** Students should understand the Control Unit as one of the main components of the CPU, responsible for directing the operation of the processor. They should learn that the Control Unit acts like a traffic cop or manager, telling other components what to do and when to do it. Students should understand how the Control Unit manages the fetch-decode-execute cycle by fetching instructions from memory, determining what operation needs to be performed, and directing other components to execute the instruction. They should learn how the Control Unit coordinates with other CPU components by sending control signals to the ALU to perform operations, to registers to temporarily store data, and to the bus system to transfer information. Examples should include how the Control Unit directs the sequence of operations in simple tasks like adding two numbers or comparing values.

**2.2.6 Identify and explain the function of the Arithmetic Logic Unit (ALU)**
- Define the ALU as a component of the CPU
- Explain the arithmetic operations performed by the ALU
- Explain the logical operations performed by the ALU
- Understand how the ALU interacts with other CPU components

**Guidance:** Students should understand the ALU as a critical component of the CPU responsible for performing arithmetic and logical operations. They should learn that the ALU performs arithmetic operations including addition, subtraction, multiplication, and division, as well as more complex mathematical functions. Students should also understand that the ALU performs logical operations such as comparisons (equal to, not equal to, greater than, less than) and boolean operations (AND, OR, NOT, XOR). They should understand how the ALU receives data from registers, performs the requested operation, and returns the result to a register or memory location. Students should learn how the ALU interacts with other CPU components, particularly the Control Unit (which tells the ALU what operation to perform) and registers (which supply data and store results). Examples should include tracing how the ALU calculates 5+3 or determines if 10 is greater than 7.

**2.2.7 Identify and explain the function of registers**
- Define registers as small, fast storage locations within the CPU
- Explain the different types of registers and their purposes
- Understand how registers are used in the fetch-decode-execute cycle
- Compare the speed and capacity of registers with other types of memory

**Guidance:** Students should understand registers as small, high-speed storage locations directly within the CPU that hold data and instructions currently being processed. They should learn about different types of registers and their specific purposes: instruction register (holds the current instruction being executed), program counter (keeps track of the next instruction to be executed), accumulator (stores intermediate results of calculations), memory address register (holds the address of a memory location), and memory data register (holds data being transferred to or from memory). Students should understand how registers are used in the fetch-decode-execute cycle: fetching instructions into the instruction register, decoding them, using the accumulator for calculations, and updating the program counter to point to the next instruction. They should compare registers with other types of memory, understanding that registers are the fastest but have the smallest capacity, followed by cache memory, RAM, and then storage devices.

**2.2.8 Explain the bus system and how CPU components communicate**
- Define the bus system as the communication pathway in a computer
- Identify the different types of buses (data, address, control)
- Explain how buses facilitate communication between CPU components
- Understand the concept of bus width and its impact on performance

**Guidance:** Students should understand the bus system as the set of wires or conductors that transmit data and signals between different components within the CPU and between the CPU and other parts of the computer. They should learn about the three main types of buses: the data bus (carries actual data between components), the address bus (specifies the source or destination of data), and the control bus (carries control and timing signals). Students should understand how these buses facilitate communication between CPU components by providing pathways for information to travel. They should learn about bus width (the number of wires in a bus) and how it affects performance - wider buses can transmit more data simultaneously, leading to faster processing. Examples should include how a 64-bit data bus can transmit 64 bits (8 bytes) of data at once, while a 32-bit bus can only handle 32 bits (4 bytes).

**2.2.9 Explain the fetch-decode-execute cycle in detail**
- Define the fetch-decode-execute cycle as the fundamental process of CPU operation
- Explain each stage of the cycle (fetch, decode, execute)
- Describe how the cycle repeats continuously during CPU operation
- Apply the fetch-decode-execute cycle to simple program examples

**Guidance:** Students should understand the fetch-decode-execute cycle as the fundamental process by which CPUs execute instructions. They should learn about each stage in detail: fetch (retrieving the next instruction from memory using the program counter and address bus), decode (interpreting the instruction to determine what operation needs to be performed), and execute (carrying out the instruction using the ALU, registers, and other components as needed). Students should understand that this cycle repeats continuously while the computer is running, with the program counter typically being updated to point to the next instruction after each cycle. They should be able to apply this cycle to simple program examples, such as adding two numbers: fetching the ADD instruction, decoding that it needs to add two values, executing the addition in the ALU, and storing the result in a register or memory location.

**2.2.10 Understand different CPU architectures (CISC vs RISC)**
- Define CISC (Complex Instruction Set Computer) architecture
- Define RISC (Reduced Instruction Set Computer) architecture
- Compare the characteristics and approaches of CISC and RISC
- Identify examples of processors using each architecture

**Guidance:** Students should understand the two main approaches to CPU architecture: CISC and RISC. They should learn that CISC (Complex Instruction Set Computer) architecture uses a large set of complex instructions that can perform multiple low-level operations in a single instruction. This approach aims to accomplish more with each instruction, potentially reducing the number of instructions per program. In contrast, RISC (Reduced Instruction Set Computer) architecture uses a small set of simple, highly optimized instructions that typically execute in one clock cycle. This approach focuses on simplifying instructions to improve efficiency. Students should compare the characteristics of each approach: CISC emphasizes hardware complexity to simplify software, while RISC emphasizes software complexity to simplify hardware. They should identify examples of processors using each architecture, such as Intel x86 processors (CISC) and ARM processors (RISC), and understand how modern processors often incorporate elements of both approaches.

**2.2.11 Explain clock speed and how it's measured**
- Define clock speed as the rate at which a CPU executes instructions
- Explain how clock speed is measured (Hertz, megahertz, gigahertz)
- Understand the relationship between clock speed and processing power
- Explain the limitations of clock speed as a performance indicator

**Guidance:** Students should understand clock speed as the rate at which a CPU can execute instructions, measured in cycles per second. They should learn that clock speed is measured in Hertz (Hz), with modern CPUs typically measured in gigahertz (GHz), where 1 GHz equals one billion cycles per second. Students should understand the relationship between clock speed and processing power - generally, a higher clock speed means more instructions can be executed per second, leading to faster performance. However, they should also understand the limitations of clock speed as a performance indicator, including that different CPUs may accomplish different amounts of work per clock cycle (measured in instructions per cycle or IPC), and that factors like architecture, number of cores, and efficiency also significantly impact overall performance. Examples should include comparing a 3.0 GHz CPU with a 2.5 GHz CPU, explaining that while the 3.0 GHz CPU can potentially execute more instructions per second, other factors may affect their relative performance.

**2.2.12 Understand the role of cores and multi-core processing**
- Define a CPU core as an independent processing unit
- Explain the benefits of multi-core processors
- Understand how multiple cores work together to process tasks
- Compare single-core and multi-core performance in different scenarios

**Guidance:** Students should understand a CPU core as an independent processing unit within a CPU that can read and execute program instructions. They should learn about the benefits of multi-core processors, including the ability to perform multiple tasks simultaneously (true parallel processing), improved performance for multi-threaded applications, and better power efficiency compared to increasing single-core clock speeds. Students should understand how multiple cores work together through techniques like symmetric multiprocessing (where the operating system distributes workloads across available cores) and specialized communication pathways between cores. They should compare single-core and multi-core performance in different scenarios, recognizing that while single-core performance is important for tasks that can't be parallelized, multi-core processors excel at multitasking and running applications designed to use multiple cores. Examples should include how a quad-core processor can handle web browsing, music playback, and file downloading simultaneously with better performance than a single-core processor.

**2.2.13 Explain cache memory and its levels (L1, L2, L3)**
- Define cache memory as fast, small memory located close to the CPU
- Explain the purpose of cache memory in improving CPU performance
- Identify and describe the different levels of cache memory (L1, L2, L3)
- Understand the relationship between cache size, speed, and distance from the CPU

**Guidance:** Students should understand cache memory as a small amount of very fast memory located close to the CPU that stores frequently accessed data and instructions. They should learn that the purpose of cache memory is to reduce the time the CPU spends waiting for data from slower main memory (RAM), thereby improving overall system performance. Students should be able to identify and describe the different levels of cache memory: L1 cache (smallest but fastest, typically built directly into each CPU core), L2 cache (larger and slightly slower than L1, may be shared between cores or dedicated to individual cores), and L3 cache (largest and slowest of the cache levels, typically shared among all CPU cores). They should understand the relationship between cache size, speed, and distance from the CPU - smaller caches are faster and closer to the CPU cores, while larger caches are slower but can hold more data. Examples should include how cache memory speeds up common operations like repeatedly accessing the same data or executing loops in programs.

**2.2.14 Understand instruction sets and their impact on performance**
- Define an instruction set as the collection of commands a CPU can understand and execute
- Explain the relationship between instruction sets and software compatibility
- Compare different instruction set architectures (x86, ARM, etc.)
- Understand how instruction set design affects CPU performance and efficiency

**Guidance:** Students should understand an instruction set as the complete collection of commands that a CPU can recognize and execute, representing the interface between software and hardware. They should learn about the relationship between instruction sets and software compatibility - software must be compiled or translated to match the instruction set of the CPU it runs on, which is why software for x86 processors (like most Windows PCs) doesn't run on ARM processors (like most smartphones) without translation. Students should compare different instruction set architectures, such as x86 (used in most desktop and laptop computers, characterized by complex instructions and variable instruction lengths) and ARM (used in most mobile devices, characterized by simple, fixed-length instructions and power efficiency). They should understand how instruction set design affects CPU performance and efficiency, including trade-offs between complexity, power consumption, and execution speed. Examples should include how ARM's simpler instruction set contributes to better battery life in mobile devices, while x86's more complex instructions can lead to better performance in certain compute-intensive tasks.

**2.2.15 Identify other factors affecting CPU performance**
- Explain the impact of manufacturing process (measured in nanometers)
- Understand the role of transistor count in CPU capabilities
- Explain how thermal design power (TDP) affects CPU performance
- Identify how front-side bus speed and memory bandwidth affect CPU performance

**Guidance:** Students should understand various factors beyond clock speed and core count that affect CPU performance. They should learn about the manufacturing process (measured in nanometers), which refers to the size of transistors on the CPU - smaller processes allow for more transistors in the same space, reducing power consumption and heat generation while potentially increasing performance. Students should understand the role of transistor count in CPU capabilities, with more transistors generally allowing for more features, larger caches, and more cores. They should explain thermal design power (TDP) as the maximum amount of heat a CPU is designed to generate under load, and how higher TDP typically allows for higher performance but requires better cooling. Students should also identify how front-side bus speed (the data transfer rate between the CPU and other components) and memory bandwidth (the rate at which data can be read from or written to memory) can affect CPU performance, particularly in tasks that involve frequent memory access. Examples should include comparing CPUs with different manufacturing processes (e.g., 7nm vs 14nm) and explaining how this affects their performance and efficiency.

**2.2.16 Compare and evaluate different CPU specifications**
- Explain how to interpret CPU model numbers and naming conventions
- Compare CPUs based on relevant specifications for different use cases
- Understand benchmarking and its role in evaluating CPU performance
- Apply knowledge of CPU specifications to make informed purchasing decisions

**Guidance:** Students should learn how to interpret CPU model numbers and naming conventions from major manufacturers like Intel (Core i3, i5, i7, i9) and AMD (Ryzen 3, 5, 7, 9), understanding what these numbers indicate about performance levels and target markets. They should learn to compare CPUs based on specifications relevant to different use cases - for example, prioritizing single-core performance for gaming, multi-core performance for video editing, and power efficiency for mobile devices. Students should understand benchmarking as the process of running standardized tests to measure and compare CPU performance across different tasks and applications. They should learn about various types of benchmarks, including synthetic benchmarks (designed specifically to test performance) and real-world benchmarks (measuring performance in actual applications). Students should apply their knowledge of CPU specifications to make informed purchasing decisions, considering factors like intended use, budget, and compatibility with other system components.

**2.2.17 Identify AMD as a major CPU manufacturer and their history**
- Recognize AMD as a major producer of CPUs and other semiconductors
- Explain key milestones in AMD's history
- Understand AMD's position in the CPU market relative to competitors
- Identify AMD's contributions to CPU technology development

**Guidance:** Students should recognize AMD (Advanced Micro Devices) as one of the world's leading producers of CPUs, GPUs, and other semiconductor products. They should learn about key milestones in AMD's history, including its founding in 1969, the development of early microprocessors in the 1970s, the introduction of the Am386 and Am486 processors as alternatives to Intel's products, the release of the Athlon K7 processor in 1999 (which outperformed competing Intel processors), the acquisition of ATI Technologies in 2006 (expanding into graphics processing), and the introduction of the Ryzen architecture in 2017 (which marked a significant comeback in CPU performance and competitiveness). Students should understand AMD's position in the CPU market as the primary competitor to Intel, with varying market share over time but consistently pushing innovation and price competition. They should identify AMD's contributions to CPU technology development, including early adoption of 64-bit computing in consumer processors, multi-core processors, and innovations in chiplet design that have influenced the entire industry.

**2.2.18 Explain AMD's processor families and their target markets**
- Identify AMD's main processor families (Ryzen, EPYC, Threadripper)
- Explain the target markets for each processor family
- Compare the features and capabilities of different AMD processor lines
- Understand how AMD positions its products against competitors' offerings

**Guidance:** Students should be able to identify AMD's main processor families and their intended markets. They should learn about Ryzen processors, designed for mainstream consumer desktop and laptop computers, offering a balance of performance and value for everyday computing, gaming, and content creation. Students should understand EPYC processors as AMD's server and data center processors, optimized for high core counts, reliability, and enterprise features like virtualization and security. They should recognize Threadripper as AMD's high-end desktop (HEDT) platform, targeting professionals and enthusiasts who need extreme multi-core performance for demanding workloads like 3D rendering, scientific computing, and heavy multitasking. Students should compare the features and capabilities of these different lines, noting how Ryzen focuses on balanced performance and efficiency, EPYC on scalability and reliability for enterprise environments, and Threadripper on maximum core count and performance for specialized applications. They should understand how AMD positions these products against competitors' offerings, typically emphasizing core count, multithreading performance, and value compared to equivalent Intel processors.

**2.2.19 Understand AMD's architectural innovations**
- Explain AMD's Zen microarchitecture and its evolution
- Understand AMD's chiplet design approach
- Identify key innovations in AMD's CPU designs
- Explain how these innovations have influenced the industry

**Guidance:** Students should learn about AMD's Zen microarchitecture, first introduced in 2017, which marked a significant turning point in AMD's competitiveness. They should understand the evolution of Zen through multiple generations (Zen, Zen+, Zen 2, Zen 3, Zen 4), with each bringing improvements in instructions per clock (IPC), power efficiency, and features. Students should understand AMD's chiplet design approach, which involves using multiple smaller silicon dies (chiplets) connected via high-speed interconnects rather than a single large monolithic die. This approach allows for better yields (more working chips per silicon wafer), improved scalability, and cost-effectiveness. Students should identify key innovations in AMD's CPU designs, including simultaneous multithreading (SMT), precision boost (automatic overclocking), and Infinity Fabric (the high-speed interconnect technology connecting chiplets). They should explain how these innovations have influenced the industry, with AMD's chiplet approach being adopted by competitors and its focus on core count and multithreading performance pushing the entire market toward higher core counts in consumer processors.

**2.2.20 Explain AMD-specific technologies and features**
- Explain AMD's Simultaneous Multithreading (SMT) technology
- Understand AMD's Precision Boost and Precision Boost Overdrive
- Explain AMD's Infinity Cache technology
- Identify other AMD-specific technologies and their benefits

**Guidance:** Students should learn about AMD's Simultaneous Multithreading (SMT) technology, which allows each CPU core to handle two instruction threads simultaneously, improving performance in multi-threaded applications. They should understand AMD's Precision Boost, a technology that automatically increases clock speeds when thermal and power conditions allow, providing better performance during demanding tasks, and Precision Boost Overdrive, which allows enthusiasts to push performance even further with appropriate cooling. Students should explain AMD's Infinity Cache technology, a large pool of high-speed cache memory introduced with the RDNA 2 GPU architecture and later adapted for CPUs, which significantly reduces latency and improves performance in memory-intensive tasks. They should identify other AMD-specific technologies, such as SenseMI (a collection of sensing technologies that enable features like Precision Boost), Ryzen Master (software for CPU monitoring and tuning), and StoreMI (technology that combines SSD and HDD storage for improved performance). For each technology, students should understand the specific benefits it provides to users, such as better gaming performance, improved productivity, or enhanced system responsiveness.

**2.2.21 Compare AMD CPUs with competitors' products**
- Compare AMD Ryzen processors with Intel Core processors
- Compare AMD EPYC processors with Intel Xeon processors
- Evaluate the strengths and weaknesses of AMD CPUs relative to competitors
- Understand how AMD's competitive position has evolved over time

**Guidance:** Students should be able to compare AMD processors with competing products, primarily from Intel. They should compare AMD Ryzen processors with Intel Core processors, understanding that Ryzen typically offers more cores and threads at a given price point, while Intel often maintains advantages in single-core performance and power efficiency. Students should compare AMD EPYC processors with Intel Xeon processors, noting that EPYC generally offers higher core counts and memory bandwidth, while Xeon has traditionally held advantages in certain enterprise features and software optimization. They should evaluate the strengths and weaknesses of AMD CPUs relative to competitors, recognizing that AMD's strengths typically include multi-core performance, value (performance per dollar), and platform features, while potential weaknesses might include higher power consumption compared to equivalent Intel processors and historically lower adoption of certain professional software. Students should understand how AMD's competitive position has evolved over time, from periods of strong competition (like the Athlon era) to periods of struggling to keep up with Intel (roughly 2006-2017), to their current position of strong competition following the introduction of the Ryzen architecture.



# Topic 3: Memory and Storage Systems

Students will be assessed on their ability to:

**3.3.1 Distinguish between memory and storage in computer systems**
- Define memory as temporary, fast-access data storage used during active processing
- Define storage as permanent, high-capacity data retention for files and programs
- Compare the characteristics of memory and storage (volatility, speed, capacity, cost)
- Explain the complementary roles of memory and storage in computer operation

**Guidance:** Students should understand the fundamental distinction between memory and storage in computer systems. Memory refers to temporary, fast-access data storage that holds information actively being used by the CPU, losing its contents when power is removed (volatility). Storage refers to permanent, high-capacity data retention that maintains information even when power is off (non-volatile). Students should compare their characteristics: memory is faster but more expensive per unit and has lower capacity, while storage is slower but cheaper per unit with higher capacity. They should explain how these components work together - data and programs are loaded from storage into memory for processing, and results are saved from memory back to storage. Examples should include opening a document (loaded from storage to memory), editing it (manipulated in memory), and saving it (written back to storage).

**3.3.2 Classify different types of computer memory**
- Identify primary memory (volatile memory used directly by the CPU)
- Identify secondary memory (non-volatile storage devices)
- Identify cache memory (high-speed memory between CPU and main memory)
- Explain the purpose and characteristics of each memory type

**Guidance:** Students should be able to classify memory into distinct categories based on function and characteristics. Primary memory includes RAM and other volatile memory directly accessible by the CPU, used for active processing. Secondary memory refers to non-volatile storage devices like hard drives and SSDs that provide long-term data retention. Cache memory is a small amount of very fast memory positioned between the CPU and main memory to reduce access time. Students should explain the purpose of each type: primary memory provides working space for active programs and data; secondary memory offers permanent storage for files, applications, and operating systems; cache memory improves system performance by storing frequently accessed data closer to the CPU. They should understand the characteristics that distinguish these types, including speed, volatility, capacity, cost, and typical use cases.

**3.3.3 Explain the function and characteristics of RAM**
- Define RAM (Random Access Memory) as the main working memory of a computer
- Explain the volatile nature of RAM and its implications
- Identify the key functions of RAM in computer systems
- Describe how RAM capacity affects system performance

**Guidance:** Students should understand RAM as the primary working memory in computers, where data and programs are stored while actively being used by the CPU. They should explain that RAM is volatile, meaning it loses all stored data when power is turned off, which is why computers need to load data from storage into RAM each time they start up. Students should identify key functions of RAM, including providing temporary storage for the operating system, applications, and data currently in use; allowing quick access to information for the CPU; and enabling multitasking by keeping multiple programs readily available. They should describe how RAM capacity affects system performance - insufficient RAM forces the system to use slower storage as virtual memory, causing significant slowdowns, while adequate RAM allows smooth operation of multiple applications and handling of large files. Examples should include how upgrading from 4GB to 8GB or 16GB of RAM can improve performance in tasks like gaming, video editing, and running multiple applications simultaneously.

**3.3.4 Identify and compare different types of RAM**
- Identify DRAM (Dynamic RAM) as the most common type of RAM
- Identify SRAM (Static RAM) and explain its use in cache memory
- Compare different generations of RAM (DDR3, DDR4, DDR5)
- Explain how RAM specifications affect performance

**Guidance:** Students should be able to identify and compare different types of RAM technologies. They should recognize DRAM (Dynamic RAM) as the most common type of main memory, which stores each bit of data in a separate capacitor within an integrated circuit, requiring periodic refreshing to maintain data. Students should identify SRAM (Static RAM) as a type of memory that uses flip-flops to store each bit, doesn't need refreshing, is faster than DRAM, but is more expensive and less dense, making it suitable for cache memory rather than main memory. They should compare different generations of Double Data Rate (DDR) RAM: DDR3 (older generation, lower bandwidth, higher voltage), DDR4 (current mainstream standard, improved bandwidth and efficiency), and DDR5 (newest generation with significantly higher bandwidth and lower power consumption). Students should explain how RAM specifications affect performance, including data transfer rate (measured in MT/s), capacity (measured in GB), latency (measured in CAS timing), and bandwidth (measured in GB/s), and how these factors impact overall system responsiveness and capability.

**3.3.5 Explain the function and types of ROM**
- Define ROM (Read-Only Memory) as non-volatile memory with fixed contents
- Identify different types of ROM (PROM, EPROM, EEPROM, Flash memory)
- Explain the purpose of ROM in computer systems
- Compare ROM with RAM in terms of function and characteristics

**Guidance:** Students should understand ROM as non-volatile memory whose contents are fixed and cannot be modified (or only modified with difficulty) during normal operation. They should identify different types of ROM: PROM (Programmable ROM), which can be programmed once after manufacture; EPROM (Erasable Programmable ROM), which can be erased by exposure to UV light and reprogrammed; EEPROM (Electrically Erasable Programmable ROM), which can be erased and reprogrammed electrically; and Flash memory, a modern type of EEPROM that can be erased and rewritten in blocks. Students should explain the purpose of ROM in computer systems, including storing firmware (such as the BIOS or UEFI in computers), boot-up instructions, and other critical data that must remain unchanged when power is off. They should compare ROM with RAM, noting that ROM is non-volatile while RAM is volatile; ROM is typically read-only while RAM is read-write; ROM is slower than RAM; and ROM is used for permanent storage of essential data while RAM is used for temporary working storage.

**3.3.6 Explain cache memory and its levels**
- Define cache memory as high-speed memory that stores frequently accessed data
- Identify the different levels of cache memory (L1, L2, L3)
- Explain the purpose and characteristics of each cache level
- Understand how cache memory improves system performance

**Guidance:** Students should understand cache memory as a small amount of very fast memory that stores copies of frequently accessed data from main memory, allowing the CPU to access this data more quickly than if it had to retrieve it from the main RAM. They should identify the different levels of cache memory: L1 cache (smallest but fastest, typically built directly into each CPU core); L2 cache (larger and slightly slower than L1, may be dedicated to each core or shared between cores); and L3 cache (largest and slowest of the cache levels, typically shared among all CPU cores). Students should explain the purpose and characteristics of each cache level: L1 cache provides the fastest access to the most critical data; L2 cache offers a balance between speed and capacity; L3 cache provides a large pool of frequently used data accessible to all cores. They should understand how cache memory improves system performance by reducing the time the CPU spends waiting for data from slower main memory, particularly when the same data is accessed repeatedly. Examples should include how cache memory speeds up operations like looping through the same data or frequently accessing the same program instructions.

**3.3.7 Explain virtual memory and its implementation**
- Define virtual memory as a memory management technique that creates an illusion of more memory
- Explain how virtual memory uses storage as an extension of RAM
- Understand the role of paging in virtual memory systems
- Explain the impact of virtual memory on system performance

**Guidance:** Students should understand virtual memory as a memory management technique that provides an "idealized abstraction" of the storage resources actually available on a given machine, creating the illusion to users of a very large (main) memory. They should explain how virtual memory uses storage devices (like hard drives or SSDs) as an extension of physical RAM by temporarily transferring data that isn't actively being used from RAM to storage, and bringing it back when needed. Students should understand the role of paging in virtual memory systems, where memory is divided into fixed-size blocks called pages, and the system moves these pages between physical RAM and storage as needed. They should explain the impact of virtual memory on system performance: while it allows systems to run larger applications and more programs than would be possible with physical RAM alone, excessive use of virtual memory (thrashing) can significantly slow down the system because storage devices are much slower than RAM. Examples should include how a system with limited RAM might use virtual memory to run large applications, but with noticeable performance degradation compared to having sufficient physical RAM.

**3.3.8 Explain the memory hierarchy and its importance**
- Define the memory hierarchy as the arrangement of memory systems by speed, cost, and capacity
- Identify the levels of the memory hierarchy (registers, cache, RAM, storage)
- Explain the trade-offs between different levels of the hierarchy
- Understand how the memory hierarchy affects overall system performance

**Guidance:** Students should understand the memory hierarchy as the tiered arrangement of different types of memory systems based on their speed, cost, and capacity. They should identify the levels of the memory hierarchy in order from fastest to slowest: registers (extremely fast, small capacity, built into the CPU); cache memory (very fast, small capacity, close to the CPU); RAM (fast, moderate capacity, main working memory); and storage devices (slowest, large capacity, permanent storage). Students should explain the trade-offs between different levels of the hierarchy: faster memory is more expensive and has less capacity, while slower memory is cheaper and has greater capacity. They should understand how the memory hierarchy affects overall system performance by providing a balance between speed and cost - frequently accessed data is kept in faster, more expensive memory, while less frequently accessed data is kept in slower, cheaper memory. Examples should include how a well-designed memory hierarchy allows a computer to operate efficiently by keeping the most important data in the fastest memory while still providing large storage capacity for less critical data.

**3.3.9 Classify different types of storage devices**
- Identify magnetic storage devices (HDD, tape)
- Identify solid-state storage devices (SSD, USB flash drives)
- Identify optical storage devices (CD, DVD, Blu-ray)
- Explain the characteristics and use cases of each storage type

**Guidance:** Students should be able to classify storage devices into different categories based on their technology and characteristics. They should identify magnetic storage devices, which use magnetization to store data, including Hard Disk Drives (HDDs) that use spinning magnetic platters and tape drives that use magnetic tape. Students should identify solid-state storage devices, which use flash memory with no moving parts, including Solid State Drives (SSDs) and USB flash drives. They should also identify optical storage devices, which use lasers to read and write data on optical discs, including CDs, DVDs, and Blu-ray discs. Students should explain the characteristics and use cases of each storage type: magnetic storage offers high capacity at low cost but slower speeds and mechanical vulnerability; solid-state storage provides faster speeds, better durability, and lower power consumption but at a higher cost per gigabyte; optical storage offers inexpensive, removable media but limited capacity and slower speeds. Examples should include using HDDs for bulk data storage, SSDs for operating systems and applications, and optical discs for software distribution and media playback.

**3.3.10 Explain the function and characteristics of Hard Disk Drives (HDDs)**
- Define HDDs as storage devices that use magnetic storage on rotating platters
- Identify the main components of an HDD (platters, read/write heads, actuator arm)
- Explain how data is stored and accessed on HDDs
- Compare the advantages and limitations of HDDs

**Guidance:** Students should understand HDDs as storage devices that store data on rapidly rotating magnetic platters using magnetic recording. They should identify the main components of an HDD: platters (circular disks coated with magnetic material where data is stored); read/write heads (devices that magnetize spots on the platter surface to write data and detect magnetic fields to read data); and the actuator arm (moves the read/write heads across the platters). Students should explain how data is stored and accessed on HDDs - data is written in concentric circles called tracks, which are divided into sectors, and the read/write heads move across the spinning platters to access specific locations. They should compare the advantages of HDDs (high capacity, low cost per gigabyte, long-term reliability for archival storage) with their limitations (slower access times due to mechanical components, vulnerability to physical shock, higher power consumption than solid-state storage). Examples should include using HDDs for storing large media collections, backups, and archival data where access speed is less critical than capacity and cost.

**3.3.11 Explain the function and characteristics of Solid State Drives (SSDs)**
- Define SSDs as storage devices that use flash memory with no moving parts
- Identify the types of flash memory used in SSDs (SLC, MLC, TLC, QLC)
- Explain how data is stored and accessed on SSDs
- Compare the advantages and limitations of SSDs

**Guidance:** Students should understand SSDs as storage devices that use flash memory chips to store data persistently, with no moving mechanical components. They should identify the types of flash memory used in SSDs based on bits per cell: SLC (Single-Level Cell, storing 1 bit per cell, highest endurance and performance); MLC (Multi-Level Cell, storing 2 bits per cell, balanced performance and cost); TLC (Triple-Level Cell, storing 3 bits per cell, consumer-oriented); and QLC (Quad-Level Cell, storing 4 bits per cell, highest density but lower endurance). Students should explain how data is stored and accessed on SSDs - data is written to memory cells by trapping electrons in floating gates, and read by sensing the charge level of these cells, with a controller managing wear leveling and error correction. They should compare the advantages of SSDs (much faster access times than HDDs, no moving parts making them more durable, lower power consumption, silent operation) with their limitations (higher cost per gigabyte, limited write cycles, potential for performance degradation as they fill up). Examples should include using SSDs for operating systems, applications, and frequently accessed data where speed and responsiveness are important.

**3.3.12 Explain different storage interfaces and form factors**
- Identify common storage interfaces (SATA, NVMe, SAS)
- Explain the characteristics and performance of different interfaces
- Identify common storage form factors (2.5", 3.5", M.2, U.2)
- Understand how interface and form factor affect storage performance and compatibility

**Guidance:** Students should be able to identify and explain different storage interfaces that connect storage devices to computer systems. They should recognize common interfaces including SATA (Serial ATA, a standard interface for connecting storage devices to motherboards, widely used for HDDs and SSDs); NVMe (Non-Volatile Memory Express, a protocol designed specifically for SSDs that connects directly to the PCIe interface for much higher speeds); and SAS (Serial Attached SCSI, an enterprise-grade interface offering higher reliability and performance than SATA). Students should explain the characteristics and performance of these interfaces, noting that SATA typically offers up to 600 MB/s, while NVMe can provide multiple GB/s of bandwidth. They should identify common storage form factors: 3.5" (standard size for desktop HDDs); 2.5" (standard size for laptop HDDs and many SSDs); M.2 (a small, rectangular form factor for SSDs that plug directly into the motherboard); and U.2 (a form factor for enterprise SSDs that supports hot-swapping). Students should understand how interface and form factor affect storage performance and compatibility, including how the interface determines maximum speed and how the form factor determines physical compatibility with different computer systems.

**3.3.13 Explain storage technologies beyond HDDs and SSDs**
- Identify emerging storage technologies (Optane, 3D XPoint, DNA storage)
- Explain the principles behind these new storage technologies
- Compare the characteristics of emerging technologies with traditional storage
- Understand the potential applications of new storage technologies

**Guidance:** Students should be able to identify and explain emerging storage technologies that go beyond traditional HDDs and SSDs. They should recognize technologies like Intel Optane and 3D XPoint, which are memory technologies that fill the gap between DRAM and NAND flash by offering non-volatile storage with speeds much closer to DRAM than traditional SSDs. Students should also be aware of experimental technologies like DNA storage, which uses synthetic DNA molecules to store digital data at extremely high densities. They should explain the principles behind these technologies: 3D XPoint works by changing the resistance of memory cells through a process called bulk resistance change, while DNA storage encodes binary data into the four nucleotide bases of DNA (A, C, G, T). Students should compare the characteristics of these emerging technologies with traditional storage, noting advantages like much higher speed (for 3D XPoint) or incredible density and longevity (for DNA storage), as well as current limitations like high cost and technical challenges. They should understand potential applications, such as using 3D XPoint for high-performance caching or memory expansion, and DNA storage for extremely long-term archival of massive amounts of data.

**3.3.14 Explain RAID (Redundant Array of Independent Disks) technology**
- Define RAID as a data storage virtualization technology that combines multiple drives
- Identify common RAID levels (RAID 0, RAID 1, RAID 5, RAID 6, RAID 10)
- Explain the purpose and characteristics of each RAID level
- Understand the trade-offs between performance, capacity, and redundancy in RAID

**Guidance:** Students should understand RAID as a data storage virtualization technology that combines multiple physical disk drive components into one or more logical units for the purposes of data redundancy, performance improvement, or both. They should identify common RAID levels: RAID 0 (striping, which splits data across multiple drives for improved performance but no redundancy); RAID 1 (mirroring, which duplicates data across multiple drives for redundancy but no capacity improvement); RAID 5 (striping with distributed parity, offering both performance improvement and redundancy with one drive failure tolerance); RAID 6 (striping with double distributed parity, similar to RAID 5 but with two drive failure tolerance); and RAID 10 (combination of RAID 1 and RAID 0, offering both performance and redundancy through mirroring and striping). Students should explain the purpose and characteristics of each RAID level, including how they distribute data, their redundancy features, and their performance characteristics. They should understand the trade-offs between performance, capacity, and redundancy in different RAID configurations, recognizing that increased redundancy typically comes at the cost of usable capacity and sometimes performance. Examples should include using RAID 0 for performance-critical applications where data loss is acceptable, RAID 1 for critical systems where data integrity is paramount, and RAID 5 or 6 for servers that need both performance and data protection.

**3.3.15 Explain storage management concepts**
- Explain partitioning and its purpose in storage management
- Describe file systems and their role in organizing data
- Explain the concept of logical volume management
- Understand storage virtualization and its benefits

**Guidance:** Students should understand key concepts in storage management that help organize and optimize storage resources. They should explain partitioning as the division of a physical storage device into logical sections, each of which can be formatted with a different file system and function as a separate storage volume. Students should describe file systems as the methods and data structures that an operating system uses to control how data is stored and retrieved on storage devices, including common file systems like NTFS (Windows), APFS (macOS), ext4 (Linux), and FAT32 (cross-platform). They should explain the concept of logical volume management (LVM), which provides a higher-level view of storage resources that can span multiple physical devices, allowing for more flexible allocation, resizing, and management of storage space. Students should understand storage virtualization as the process of pooling physical storage from multiple devices into what appears to be a single storage device managed from a central console, with benefits including improved utilization, simplified management, and advanced features like snapshots and replication. Examples should include how partitioning allows multiple operating systems on one drive, how file systems determine how files are organized and accessed, and how storage virtualization enables features like thin provisioning (allocating storage space as needed rather than upfront).

**3.3.16 Explain storage performance metrics and optimization**
- Identify key storage performance metrics (IOPS, throughput, latency)
- Explain factors that affect storage performance
- Understand techniques for optimizing storage performance
- Apply storage performance concepts to real-world scenarios

**Guidance:** Students should be able to identify and explain key storage performance metrics used to evaluate and compare storage devices. They should recognize IOPS (Input/Output Operations Per Second) as a measure of how many read/write operations a storage device can perform in one second, which is particularly important for transactional workloads. Students should explain throughput (or bandwidth) as the amount of data that can be transferred to or from storage per second, typically measured in MB/s or GB/s, which is important for large file transfers. They should also explain latency as the time delay between a request for data and the beginning of data transfer, typically measured in milliseconds, which affects overall system responsiveness. Students should understand factors that affect storage performance, including the storage technology (HDD vs. SSD), interface type (SATA vs. NVMe), file system, fragmentation, and queue depth. They should learn techniques for optimizing storage performance, such as using SSDs for frequently accessed data, implementing RAID for performance improvement, partitioning strategically, and maintaining adequate free space. Students should apply these concepts to real-world scenarios, such as selecting appropriate storage for a database server (high IOPS), a video editing workstation (high throughput), or an operating system (low latency).



# Topic 4: Graphics Processing Units (GPUs)

Students will be assessed on their ability to:

**4.4.1 Define the GPU and its role in computer systems**
- Define the Graphics Processing Unit (GPU) as a specialized electronic circuit designed to rapidly manipulate and alter memory to accelerate the creation of images, videos, and animations
- Explain the historical development of GPUs from simple display controllers to modern parallel processors
- Describe the primary role of GPUs in computer systems for rendering graphics and visual data
- Understand the relationship between GPUs and other system components

**Guidance:** Students should understand the GPU as a specialized processor initially designed to accelerate graphics rendering but now used for a wide range of computationally intensive tasks. They should learn about the evolution of GPUs from early display controllers in the 1980s through the introduction of 3D accelerators in the 1990s to modern general-purpose parallel processors. Students should be able to explain how GPUs work in conjunction with CPUs - while CPUs handle general-purpose computing tasks, GPUs offload graphics processing and parallelizable computations. They should understand the physical integration of GPUs in computer systems, including integrated GPUs (built into the CPU or motherboard) and discrete GPUs (separate cards installed in expansion slots). Students should recognize how GPUs communicate with the system through the motherboard and receive data from the CPU to process visual information and return the results for display.

**4.4.2 Compare GPUs and CPUs in terms of architecture and functionality**
- Identify the architectural differences between GPUs and CPUs
- Explain the concept of parallel processing in GPUs versus sequential processing in CPUs
- Compare the core designs of GPUs (many simple cores) and CPUs (fewer complex cores)
- Analyze scenarios where GPUs outperform CPUs and vice versa

**Guidance:** Students should understand the fundamental architectural differences between GPUs and CPUs. They should learn that CPUs are designed for sequential processing with few powerful cores optimized for low-latency operations and complex decision-making, while GPUs are designed for parallel processing with thousands of simpler cores optimized for high-throughput computations. Students should explain how CPU cores are more complex and capable of handling diverse tasks, with features like branch prediction, out-of-order execution, and large caches, while GPU cores are simpler and more numerous, designed to perform the same operation on multiple data points simultaneously. They should analyze scenarios where each excels: CPUs for tasks requiring complex decision-making, serial operations, or tasks with dependencies between steps; GPUs for tasks that can be parallelized, such as graphics rendering, matrix operations, or training neural networks. Examples should include image processing (where a GPU can apply the same filter to thousands of pixels simultaneously) versus operating system management (which requires complex decision-making better suited to CPUs).

**4.4.3 Identify the physical characteristics and form factors of GPUs**
- Describe the physical appearance and components of discrete GPUs
- Identify different GPU form factors (full-height, half-height, single-slot, dual-slot)
- Explain the significance of GPU cooling solutions
- Understand power requirements and connections for GPUs

**Guidance:** Students should be able to describe the physical characteristics of discrete GPUs, including the printed circuit board (PCB), graphics processing chip (GPU die), video memory (VRAM) chips, power connectors, and display outputs. They should identify different form factors, including full-height cards (standard for most desktop computers), half-height cards (low-profile for small form factor systems), single-slot cards (occupying one expansion slot), and dual-slot cards (occupying two expansion slots, typically for more powerful GPUs with larger cooling solutions). Students should explain the significance of GPU cooling solutions, including passive cooling (heat sinks only), active air cooling (heat sinks with fans), liquid cooling (using liquid to transfer heat), and hybrid solutions (combining air and liquid cooling). They should understand power requirements, including standard power connectors (6-pin and 8-pin PCIe power connectors), power consumption measured in watts (TDP - Thermal Design Power), and the relationship between power requirements and performance. Examples should include how high-end gaming GPUs often require dual-slot designs with multiple fans and dedicated power connections, while basic GPUs might use single-slot designs with passive cooling and draw all power from the PCIe slot.

**4.4.4 Explain the evolution of GPU functionality beyond graphics**
- Describe the transition of GPUs from graphics-specific to general-purpose processors
- Explain the concept of General-Purpose computing on GPUs (GPGPU)
- Identify key milestones in GPU evolution for non-graphics applications
- Understand the factors that enabled GPUs to be used for general computing

**Guidance:** Students should understand how GPUs evolved from specialized graphics processors to general-purpose parallel computing devices. They should learn about the concept of GPGPU (General-Purpose computing on GPUs), which involves using GPUs for computations not traditionally handled by graphics hardware. Students should identify key milestones in this evolution, including the introduction of programmable shaders in the early 2000s, which allowed developers to write custom code for GPUs; the development of high-level programming languages for GPUs (like CUDA from NVIDIA and later OpenCL as an open standard); and the recognition of GPUs' potential for scientific computing and data processing. They should understand the factors that enabled this transition, including the inherently parallel nature of graphics processing, the increasing programmability of GPUs, and the performance advantages of GPUs for certain types of computations. Examples should include how early researchers realized that the same parallel processing power used for rendering 3D graphics could be applied to scientific simulations, financial modeling, and eventually machine learning.

**4.4.5 Identify and explain the main components of GPU architecture**
- Define streaming multiprocessors (SMs) or compute units as the fundamental processing blocks in GPUs
- Explain the function and structure of CUDA cores or stream processors
- Describe the memory hierarchy in GPUs (registers, shared memory, L1/L2 cache, VRAM)
- Explain the role of memory controllers and interconnects in GPU architecture

**Guidance:** Students should understand the main components that make up modern GPU architecture. They should define streaming multiprocessors (SMs) in NVIDIA GPUs or compute units in AMD GPUs as the fundamental building blocks that contain groups of cores and shared resources. Students should explain CUDA cores (NVIDIA terminology) or stream processors (AMD terminology) as the individual processing elements within SMs or compute units that perform the actual calculations. They should describe the memory hierarchy in GPUs, including registers (fastest, smallest memory local to each core), shared memory (fast memory shared by cores within an SM or compute unit), L1 and L2 cache (larger but slower cache memory), and VRAM (Video RAM, the largest but slowest memory, typically GDDR or HBM). Students should explain the role of memory controllers in managing data flow between the GPU and VRAM, and interconnects that connect different components within the GPU. Examples should include how a typical high-end GPU might contain multiple SMs or compute units, each with hundreds of CUDA cores or stream processors, and how the memory hierarchy is designed to keep data as close as possible to the processing elements to minimize latency.

**4.4.6 Explain how GPU architecture enables parallel processing**
- Define the SIMD (Single Instruction, Multiple Data) execution model in GPUs
- Explain the concept of warps (NVIDIA) or wavefronts (AMD)
- Describe how GPUs execute thousands of threads simultaneously
- Analyze the relationship between GPU architecture and parallel processing efficiency

**Guidance:** Students should understand how GPU architecture is specifically designed to maximize parallel processing capabilities. They should define the SIMD (Single Instruction, Multiple Data) execution model as a paradigm where the same instruction is executed simultaneously on multiple data elements, which is fundamental to GPU operation. Students should explain the concept of warps (in NVIDIA GPUs) or wavefronts (in AMD GPUs) as groups of threads (typically 32 or 64) that execute the same instruction simultaneously on different data elements. They should describe how modern GPUs can execute thousands or even tens of thousands of threads concurrently by distributing them across multiple SMs or compute units, with each SM handling multiple warps or wavefronts in a time-sliced manner. Students should analyze how GPU architecture features like the large number of simple cores, wide memory interfaces, and specialized hardware for specific operations (like texture filtering or matrix multiplication) contribute to parallel processing efficiency. Examples should include how a GPU can render a 3D scene by processing many pixels simultaneously, with each warp or wavefront handling a group of pixels with the same shader instructions but different data.

**4.4.7 Explain GPU memory systems and bandwidth**
- Define VRAM (Video RAM) and its purpose in GPU systems
- Compare different types of VRAM (GDDR, HBM)
- Explain the concept of memory bandwidth and its importance for GPU performance
- Understand how GPU memory systems are optimized for parallel access patterns

**Guidance:** Students should understand VRAM (Video RAM) as high-speed memory dedicated to the GPU, used to store textures, frame buffers, and other data needed for graphics processing and general-purpose computing. They should compare different types of VRAM, including GDDR (Graphics Double Data Rate) memory like GDDR6 and GDDR6X, which uses high-speed signaling techniques to achieve high bandwidth, and HBM (High Bandwidth Memory) like HBM2 and HBM2e, which stacks memory dies vertically and connects them through silicon interposers to achieve even higher bandwidth with lower power consumption. Students should explain memory bandwidth as the rate at which data can be read from or written to memory, typically measured in GB/s, and understand its critical importance for GPU performance, as even powerful processing cores can be limited by memory bandwidth. They should understand how GPU memory systems are optimized for parallel access patterns, including wide memory buses (384-bit or even 512-bit in high-end GPUs), techniques like memory compression to reduce bandwidth requirements, and specialized memory controllers designed to handle many simultaneous access requests. Examples should include how gaming at high resolutions with high-quality textures requires significant memory bandwidth, and how insufficient bandwidth can create bottlenecks even with powerful processing cores.

**4.4.8 Explain GPU programming models and APIs**
- Identify major GPU programming models (CUDA, OpenCL, DirectCompute, OpenGL compute shaders)
- Explain the differences between proprietary and open GPU programming standards
- Understand the concept of kernels in GPU programming
- Describe how developers optimize code for GPU execution

**Guidance:** Students should understand the different programming models and APIs used to write software that runs on GPUs. They should identify major GPU programming models, including CUDA (NVIDIA's proprietary platform for GPU computing), OpenCL (an open standard for parallel programming across CPUs, GPUs, and other processors), DirectCompute (Microsoft's API for GPU computing on Windows), and compute shaders (extensions to graphics APIs like OpenGL and DirectX that enable general-purpose computing). Students should explain the differences between proprietary standards like CUDA (which offers advanced features and performance but only works on NVIDIA hardware) and open standards like OpenCL (which works across hardware from different vendors but may not have the same level of optimization or features). They should understand the concept of kernels in GPU programming as functions that execute on the GPU and are applied to many data elements in parallel. Students should describe how developers optimize code for GPU execution, including maximizing parallelism, optimizing memory access patterns, minimizing data transfers between CPU and GPU, and utilizing specialized hardware features like tensor cores or ray tracing accelerators when available. Examples should include how a developer might write a CUDA kernel to process an image by having each thread work on one pixel, or how they might use OpenCL to write a matrix multiplication algorithm that works across GPUs from different manufacturers.

**4.4.9 Identify AMD as a major GPU manufacturer and their history**
- Recognize AMD as a leading producer of GPUs and graphics technologies
- Explain key milestones in AMD's GPU development history
- Understand AMD's acquisition of ATI and its significance
- Identify AMD's position in the GPU market relative to competitors

**Guidance:** Students should recognize AMD (Advanced Micro Devices) as one of the world's leading producers of GPUs and graphics technologies, competing primarily with NVIDIA in the discrete GPU market and with Intel in integrated graphics. They should learn about key milestones in AMD's GPU development history, including the early Radeon graphics cards in the early 2000s, the introduction of innovative features like tessellation in the HD 2000 series, the development of the Graphics Core Next (GCN) architecture that formed the basis for multiple generations, and the more recent RDNA (Radeon DNA) architecture that has significantly improved performance and efficiency. Students should understand the significance of AMD's acquisition of ATI Technologies in 2006, which brought extensive graphics expertise and intellectual property to AMD and established them as a major force in the GPU market. They should identify AMD's position in the GPU market as the primary competitor to NVIDIA, typically offering competitive performance with a focus on value, open-source support, and innovations like chiplet designs in high-end GPUs. Students should recognize that while NVIDIA often leads in the high-end market and has a stronger position in professional and data center markets, AMD has maintained strong competition in the consumer gaming market and has made significant inroads with recent GPU architectures.

**4.4.10 Explain AMD GPU architectures and their evolution**
- Identify major AMD GPU architectures (TeraScale, Graphics Core Next, RDNA)
- Explain the key features and innovations of each architecture
- Understand the evolution of AMD's approach to GPU design
- Compare the performance and efficiency improvements across AMD GPU generations

**Guidance:** Students should be able to identify and explain the major AMD GPU architectures and their evolution over time. They should learn about the TeraScale architecture (used in Radeon HD 2000 through HD 7000 series), which introduced unified shaders and was AMD's primary architecture through the late 2000s and early 2010s. Students should understand the Graphics Core Next (GCN) architecture (introduced with the HD 7000 series and used through the RX 500 series), which represented a significant redesign focused on compute capabilities, power efficiency, and scalability, forming the basis of multiple generations and also powering the GPUs in the PlayStation 4 and Xbox One consoles. They should explain the RDNA (Radeon DNA) architecture (introduced with the RX 5000 series and continuing with RDNA2 and RDNA3), which focused on improving performance per watt, reducing instruction latency, and introducing features like hardware-accelerated ray tracing. Students should understand the evolution of AMD's approach to GPU design, from the VLIW (Very Long Instruction Word) design of TeraScale to the RISC (Reduced Instruction Set Computer) approach of GCN and the further refinements in RDNA. They should compare the performance and efficiency improvements across these generations, noting how each architecture has delivered significant improvements in performance, power efficiency, and features.

**4.4.11 Explain AMD-specific GPU technologies and features**
- Explain AMD's Infinity Cache technology and its benefits
- Understand AMD's approach to ray tracing acceleration
- Identify AMD's FidelityFX Super Resolution (FSR) upscaling technology
- Explain AMD's Smart Access Memory technology

**Guidance:** Students should learn about key technologies and features specific to AMD GPUs. They should explain AMD's Infinity Cache technology (introduced with RDNA2) as a large, high-bandwidth cache that sits between the GPU cores and VRAM, significantly reducing power consumption while increasing effective memory bandwidth. Students should understand AMD's approach to ray tracing acceleration, which uses dedicated hardware in RDNA2 and RDNA3 GPUs to accelerate the intersection calculations required for ray tracing, though with a different implementation than NVIDIA's RTX technology. They should identify AMD's FidelityFX Super Resolution (FSR) as an open-source spatial upscaling technology that renders games at a lower resolution and then intelligently upscales the image, improving performance while maintaining visual quality, with versions including FSR 1.0 (spatial upscaling) and FSR 2.0/3.0 (temporal upscaling with improved quality). Students should explain AMD's Smart Access Memory technology, which allows the CPU to access the full GPU memory (VRAM) when using a compatible AMD CPU and motherboard, potentially improving performance in some games. For each technology, students should understand the technical implementation, the benefits it provides, and how it compares to competing technologies from other manufacturers.

**4.4.12 Explain AMD's ray tracing and machine learning capabilities**
- Understand AMD's implementation of hardware-accelerated ray tracing
- Explain AMD's approach to machine learning and AI workloads
- Compare AMD's ray tracing and AI capabilities with competitors
- Identify software and frameworks that leverage AMD's ML capabilities

**Guidance:** Students should understand AMD's implementation of hardware-accelerated ray tracing in their RDNA2 and RDNA3 architectures. They should learn that AMD's approach uses Ray Accelerators, specialized hardware units that speed up the intersection calculations required to determine where light rays intersect with objects in a scene. Students should explain AMD's approach to machine learning and AI workloads, which has traditionally focused on leveraging the general compute capabilities of their GPUs rather than dedicated tensor cores like those found in NVIDIA's RTX cards. They should understand that AMD GPUs can still perform well in many AI and machine learning tasks through optimized software and frameworks, particularly those that support open standards like OpenCL. Students should compare AMD's ray tracing and AI capabilities with competitors, recognizing that while NVIDIA currently leads in both areas with dedicated hardware (RT cores for ray tracing and Tensor cores for AI), AMD has been closing the gap with each architecture generation and often offers better price-to-performance ratios in certain scenarios. They should identify software and frameworks that leverage AMD's ML capabilities, including ROCm (Radeon Open Compute platform for GPU computing), TensorFlow and PyTorch with appropriate backends, and AMD's own ML frameworks and libraries.

**4.4.13 Compare AMD GPUs with competing products**
- Compare AMD Radeon GPUs with NVIDIA GeForce GPUs
- Evaluate the strengths and weaknesses of AMD GPUs relative to competitors
- Understand the concept of price-to-performance ratios in GPU selection
- Analyze market segments where AMD GPUs are particularly competitive

**Guidance:** Students should be able to compare AMD Radeon GPUs with competing products, primarily NVIDIA's GeForce GPUs. They should evaluate the relative strengths and weaknesses of AMD GPUs, including strengths like typically offering more VRAM at a given price point, strong performance in rasterization (traditional rendering), good support for open-source technologies, and competitive performance in compute-intensive tasks. Weaknesses might include higher power consumption compared to equivalent NVIDIA GPUs, historically slower adoption of ray tracing technologies, and less mature machine learning frameworks. Students should understand the concept of price-to-performance ratios in GPU selection, recognizing that while the absolute fastest GPUs might come from NVIDIA, AMD often offers better value at specific price points, particularly in the mid-range market. They should analyze market segments where AMD GPUs are particularly competitive, including budget and mid-range gaming, content creation applications that benefit from more VRAM, and Linux and open-source environments where AMD's driver support is often considered superior. Examples should include specific product comparisons like the Radeon RX 7900 XTX versus the GeForce RTX 4080, analyzing their relative performance, power consumption, features, and pricing.

**4.4.14 Explain General-Purpose computing on GPUs (GPGPU)**
- Define GPGPU as the use of GPUs for computations not traditionally handled by graphics hardware
- Explain the advantages of using GPUs for general-purpose computing
- Identify programming frameworks for GPGPU (CUDA, OpenCL, DirectCompute)
- Understand the limitations and challenges of GPGPU

**Guidance:** Students should understand GPGPU as the utilization of GPUs for computations beyond their original purpose of graphics rendering. They should explain the advantages of using GPUs for general-purpose computing, including massively parallel processing capabilities (thousands of cores working simultaneously), high memory bandwidth, and specialized hardware for certain mathematical operations. Students should identify major programming frameworks for GPGPU, including CUDA (NVIDIA's proprietary platform), OpenCL (open standard for heterogeneous computing), DirectCompute (Microsoft's API for Windows), and higher-level frameworks that build on these like TensorFlow and PyTorch for machine learning. They should understand the limitations and challenges of GPGPU, including the need to redesign algorithms to exploit parallelism, overhead of data transfer between CPU and GPU memory, programming complexity compared to CPU development, and limitations for tasks that require sequential processing or complex decision-making. Examples should include how GPGPU has revolutionized fields like scientific computing (allowing complex simulations to run much faster), machine learning (enabling the training of large neural networks), and data analysis (accelerating processing of large datasets).

**4.4.15 Explain GPU applications in artificial intelligence and machine learning**
- Understand how GPUs accelerate neural network training and inference
- Explain the role of GPUs in deep learning frameworks
- Identify specialized AI hardware features in modern GPUs
- Compare different GPU architectures for AI workloads

**Guidance:** Students should understand how GPUs have become essential for artificial intelligence and machine learning, particularly for deep learning. They should explain how GPUs accelerate neural network training by performing the massive matrix multiplications and convolutions required in parallel, significantly reducing training time from weeks or months to days or hours. Students should understand the role of GPUs in deep learning frameworks like TensorFlow, PyTorch, and MXNet, which use GPU acceleration to speed up both training and inference of neural networks. They should identify specialized AI hardware features in modern GPUs, including tensor cores (in NVIDIA RTX and AMD RDNA2/3 GPUs) that accelerate matrix operations, mixed precision capabilities that allow calculations with reduced precision for faster performance, and specialized instructions for common AI operations. Students should compare different GPU architectures for AI workloads, including how NVIDIA's CUDA platform and Tensor cores have given them an advantage in this space, while AMD has been improving their AI capabilities with each generation and offers competitive performance in many scenarios, particularly through open-source frameworks and ROCm. Examples should include how training large language models or image recognition systems would be practically impossible without the parallel processing power of modern GPUs.

**4.4.16 Explain GPU applications in scientific computing and research**
- Identify fields where GPUs are used for scientific computing
- Explain how GPUs accelerate specific types of scientific calculations
- Understand the impact of GPU acceleration on research capabilities
- Compare GPU-based solutions with traditional CPU-based approaches

**Guidance:** Students should be able to identify various scientific fields where GPUs are used for computing, including computational biology (protein folding, drug discovery), climate modeling and weather prediction, computational fluid dynamics, physics simulations, financial modeling, and astronomical data analysis. They should explain how GPUs accelerate specific types of scientific calculations, such as molecular dynamics simulations (where interactions between thousands of atoms can be calculated simultaneously), finite element analysis (where complex structures are divided into many small elements processed in parallel), and Monte Carlo simulations (where thousands of random scenarios are computed simultaneously). Students should understand the impact of GPU acceleration on research capabilities, including enabling larger and more complex simulations, reducing computation time from days or weeks to hours, allowing researchers to iterate more quickly and explore more possibilities, and making advanced computational methods accessible to more researchers with limited budgets. They should compare GPU-based solutions with traditional CPU-based approaches, recognizing that while CPUs are still necessary for many parts of scientific workflows, GPU acceleration can provide speedups of 10x, 50x, or even more for suitable algorithms, fundamentally changing what's possible in research. Examples should include how GPU-accelerated molecular dynamics simulations have accelerated drug discovery, or how climate models with higher resolution have become possible with GPU computing.

**4.4.17 Explain GPU applications in content creation and professional workflows**
- Identify content creation applications that benefit from GPU acceleration
- Explain how GPUs accelerate specific content creation tasks
- Understand the role of GPUs in real-time content creation and production
- Compare different GPU solutions for professional content creation

**Guidance:** Students should be able to identify various content creation applications that benefit from GPU acceleration, including video editing and compositing, 3D modeling and animation, visual effects, game development, graphic design, and photography. They should explain how GPUs accelerate specific content creation tasks, such as video encoding/decoding, real-time video effects playback, 3D rendering, ray tracing for realistic lighting and shadows, physics simulations, and AI-powered features like noise reduction or object selection. Students should understand the role of GPUs in real-time content creation and production, including how powerful GPUs enable real-time editing of high-resolution video, real-time 3D rendering for preview and animation, and real-time ray tracing for lighting and shadow previews. They should compare different GPU solutions for professional content creation, including NVIDIA's RTX and AMD's Radeon Pro cards, understanding that professional solutions often offer certified drivers for professional applications, higher memory capacity, and features like ECC (Error Correcting Code) memory for critical work. Students should recognize how GPU acceleration has transformed content creation workflows, enabling faster iteration times, higher quality outputs, and new creative possibilities that weren't practical with CPU-only processing. Examples should include how video editors can now work with multiple streams of 8K video in real time, or how 3D artists can see near-final quality renders while adjusting lighting and materials in real time.

**4.4.18 Explain emerging applications and future trends for GPUs**
- Identify emerging fields where GPU computing is being applied
- Explain new GPU technologies and their potential applications
- Understand the future direction of GPU development
- Analyze the potential impact of new GPU technologies on various industries

**Guidance:** Students should be able to identify emerging fields where GPU computing is being applied, including edge computing and IoT (where GPUs accelerate AI at the edge), autonomous vehicles (processing sensor data and making real-time decisions), virtual and augmented reality (rendering complex environments in real time), and blockchain and cryptocurrency (GPU-accelerated mining and validation). They should explain new GPU technologies and their potential applications, such as chiplet designs (like AMD's RDNA3) that allow for more scalable and cost-effective GPUs, dedicated hardware for ray tracing and AI, and new memory technologies that further increase bandwidth. Students should understand the future direction of GPU development, including continued increases in parallel processing power, more specialized hardware for specific workloads, better integration with other system components, and improvements in energy efficiency. They should analyze the potential impact of new GPU technologies on various industries, including how more powerful and efficient GPUs could enable new applications in healthcare (real-time medical imaging analysis), scientific research (more complex and accurate simulations), entertainment (more realistic and immersive experiences), and business (faster data processing and decision-making). Examples should include how future GPUs might enable real-time global climate modeling with high resolution, or how AI acceleration could transform medical diagnosis and treatment planning.



# Topic 5: Motherboards and System Integration

Students will be assessed on their ability to:

**5.5.1 Define the motherboard and its role in computer systems**
- Define the motherboard as the primary printed circuit board (PCB) in a computer system
- Explain the motherboard's function as the central hub connecting all computer components
- Describe how the motherboard facilitates communication between hardware components
- Understand the relationship between the motherboard and overall system functionality

**Guidance:** Students should understand the motherboard as the main circuit board that serves as the foundation of a computer system, providing the electrical connections through which other components communicate. They should learn that the motherboard houses crucial components like the CPU, RAM, and various controllers, while providing connectors for additional hardware. Students should explain how the motherboard acts as the central nervous system of the computer, facilitating data transfer between components through various buses and pathways. They should understand that the motherboard's quality, features, and compatibility directly impact the overall functionality, performance, and expandability of the computer system. Examples should include how a motherboard determines what processors and memory can be used, what expansion cards can be added, and what peripherals can be connected.

**5.5.2 Classify motherboard form factors and standards**
- Identify common motherboard form factors (ATX, microATX, Mini-ITX, EATX)
- Explain the characteristics and dimensions of each form factor
- Understand the relationship between form factor and case compatibility
- Apply knowledge of form factors to select appropriate motherboards for different scenarios

**Guidance:** Students should be able to identify and classify different motherboard form factors based on their physical dimensions, layouts, and intended uses. They should learn about ATX (Advanced Technology Extended) as the standard full-size form factor (typically 12" × 9.6"), microATX as a smaller version (typically 9.6" × 9.6"), Mini-ITX as a compact form factor (6.7" × 6.7"), and EATX (Extended ATX) as a larger form factor for high-end systems (typically 12" × 13"). Students should explain the characteristics of each form factor, including number of expansion slots, RAM slots, and connectivity options. They should understand the relationship between motherboard form factor and computer case compatibility - a case must support the motherboard's form factor to ensure proper mounting and alignment of ports. Students should apply this knowledge to select appropriate motherboards for different scenarios, such as using Mini-ITX for compact home theater PCs, ATX for standard desktops, or EATX for workstations requiring maximum expansion.

**5.5.3 Identify and explain the function of key motherboard components**
- Identify the CPU socket and explain its role in processor installation
- Identify memory slots and explain their function in RAM installation
- Identify expansion slots and explain their purpose for add-on cards
- Identify storage connectors and explain their role in connecting storage devices

**Guidance:** Students should be able to identify and explain the function of essential components found on motherboards. They should recognize the CPU socket as the specific interface designed to hold the processor, with different sockets supporting different processor types (e.g., LGA 1700 for Intel 12th-14th gen, AM5 for AMD Ryzen 7000 series). Students should identify memory slots (typically DIMM slots for desktop computers) and explain their function in connecting RAM modules to the system, with different generations supporting different types of memory (DDR4, DDR5). They should identify expansion slots (primarily PCIe slots in modern motherboards) and explain their purpose for connecting add-on cards like graphics cards, sound cards, and network adapters. Students should also identify storage connectors, including SATA ports for connecting traditional storage devices and M.2 slots for connecting high-speed NVMe SSDs. For each component, students should understand its physical appearance, location on the motherboard, and critical role in system operation.

**5.5.4 Explain the role of chipsets in motherboard functionality**
- Define the motherboard chipset as a set of electronic components that manage data flow
- Identify the main components of the chipset (Northbridge/Southbridge or Platform Controller Hub)
- Explain how the chipset determines motherboard capabilities and compatibility
- Compare different chipset tiers and their target applications

**Guidance:** Students should understand the motherboard chipset as a collection of integrated circuits that manage data flow between the CPU, memory, and peripherals. They should learn about the traditional chipset architecture with Northbridge (handling high-speed components like CPU, RAM, and PCIe) and Southbridge (handling slower components like SATA, USB, and audio), as well as the modern Platform Controller Hub (PCH) design used in current systems where most functions are integrated into a single chip. Students should explain how the chipset determines motherboard capabilities, including supported CPU generation, memory type and speed, number of USB ports, number of SATA ports, PCIe lane allocation, and overclocking support. They should compare different chipset tiers (e.g., Intel's Z-series for enthusiasts, B-series for mainstream, H-series for budget; AMD's X-series for enthusiasts, B-series for mainstream, A-series for budget) and understand their target applications based on features, overclocking support, and expansion capabilities. Examples should include selecting a Z790 motherboard for an Intel gaming system with overclocking or a B650 motherboard for a mainstream AMD Ryzen build.

**5.5.5 Explain BIOS/UEFI and its role in system initialization**
- Define BIOS (Basic Input/Output System) and UEFI (Unified Extensible Firmware Interface)
- Explain the role of BIOS/UEFI in system initialization and hardware configuration
- Identify key functions performed by BIOS/UEFI during the boot process
- Understand how BIOS/UEFI settings affect system operation and performance

**Guidance:** Students should understand BIOS as the traditional firmware interface used to initialize hardware during the boot process, and UEFI as its modern replacement with advanced features. They should explain the role of BIOS/UEFI in performing the Power-On Self-Test (POST) to check that hardware is functioning correctly, initializing system components, loading the operating system bootloader, and providing a configuration interface for system settings. Students should identify key functions performed during the boot process, including detecting and configuring hardware, establishing the system clock, managing power settings, and preparing the environment for the operating system. They should understand how BIOS/UEFI settings affect system operation and performance, including boot order configuration, enabling/disabling hardware components, memory timing settings, overclocking options, and security features like Secure Boot. Students should also recognize the advantages of UEFI over traditional BIOS, including support for larger hard drives, faster boot times, graphical user interfaces, mouse navigation, and enhanced security features.

**5.5.6 Explain power delivery systems on motherboards**
- Identify the components of motherboard power delivery systems
- Explain the function of voltage regulator modules (VRMs)
- Understand the importance of power phases in CPU power delivery
- Apply knowledge of power delivery to evaluate motherboard quality and suitability

**Guidance:** Students should be able to identify the components of motherboard power delivery systems, including voltage regulator modules (VRMs), chokes, capacitors, and power phases. They should explain the function of VRMs in converting power from the power supply to the specific voltages required by the CPU and other components, acting as a "step-down transformer" for electricity. Students should understand the concept of power phases in CPU power delivery, where multiple phases work together to provide cleaner, more stable power to the processor, with more phases generally allowing for better power efficiency, lower temperatures, and greater overclocking potential. They should learn to identify quality power delivery components by their physical characteristics, such as the number of power phases, quality of capacitors and chokes, and presence of heatsinks on VRMs. Students should apply this knowledge to evaluate motherboard quality and suitability for different applications, recognizing that high-power processors and overclocking require robust power delivery systems, while basic systems can function with simpler power designs.

**5.5.7 Explain expansion slots and their evolution**
- Identify different types of expansion slots (PCIe, AGP, PCI)
- Explain the characteristics and capabilities of each expansion slot type
- Understand the evolution of expansion slots and their impact on system performance
- Apply knowledge of expansion slots to select appropriate components and configurations

**Guidance:** Students should be able to identify different types of expansion slots found on motherboards, with primary focus on modern PCIe (Peripheral Component Interconnect Express) slots, but also including historical standards like AGP (Accelerated Graphics Port) and PCI (Peripheral Component Interconnect). They should explain the characteristics and capabilities of each slot type, including physical dimensions, bandwidth (e.g., PCIe x1, x4, x8, x16), and version (e.g., PCIe 3.0, 4.0, 5.0), with each generation doubling the bandwidth of the previous one. Students should understand the evolution of expansion slots from early standards like ISA and VESA through PCI and AGP to modern PCIe, and how this evolution has enabled increasingly powerful add-on cards and faster data transfer rates. They should apply this knowledge to select appropriate components and configurations, such as understanding that a high-end graphics card requires a PCIe x16 slot for optimal performance, or that multiple PCIe slots are needed for specialized cards like sound cards, capture cards, or additional storage controllers. Examples should include configuring a system with multiple graphics cards, selecting a motherboard with sufficient PCIe lanes for expansion needs, or understanding the impact of PCIe version on device performance.

**5.5.8 Explain motherboard connectivity options**
- Identify external connectivity options (USB, audio, video, network)
- Identify internal connectivity options (fan headers, front panel connectors, RGB headers)
- Explain the function and purpose of different connectivity options
- Apply knowledge of connectivity to configure complete computer systems

**Guidance:** Students should be able to identify and explain the various connectivity options available on motherboards. They should recognize external connectivity options, including USB ports (various generations like USB 2.0, 3.0, 3.1, 3.2, and USB-C with different speeds and capabilities), audio ports (for connecting speakers, microphones, and headphones), video outputs (HDMI, DisplayPort, VGA, DVI), and network ports (Ethernet, Wi-Fi). Students should also identify internal connectivity options, including fan headers (for connecting case fans and CPU coolers), front panel connectors (for power button, reset button, status LEDs, and front panel USB/audio), RGB headers (for controlling lighting systems), and internal headers for additional features like Thunderbolt or Wi-Fi modules. They should explain the function and purpose of each connectivity option, understanding how they enable interaction with peripheral devices and system control. Students should apply this knowledge to configure complete computer systems, ensuring that all required connections are available and properly configured, such as selecting a motherboard with sufficient USB ports for peripheral needs, appropriate audio outputs for sound systems, or necessary internal headers for case features and cooling solutions.

**5.5.9 Explain system integration principles**
- Define system integration as the process of assembling compatible components into a functional computer system
- Explain the importance of component compatibility in system integration
- Identify key considerations in system integration (power requirements, physical constraints, thermal management)
- Apply system integration principles to design balanced computer systems

**Guidance:** Students should understand system integration as the process of selecting, assembling, and configuring compatible computer components to create a functional system that meets specific requirements. They should explain the importance of component compatibility, including CPU socket compatibility with the motherboard, memory type compatibility, power supply adequacy for all components, physical fit within the case, and operating system support. Students should identify key considerations in system integration, including power requirements (ensuring the power supply can deliver sufficient wattage and has appropriate connectors), physical constraints (ensuring components fit within the case and don't interfere with each other), and thermal management (ensuring adequate cooling for all components). They should apply these principles to design balanced computer systems for different purposes, such as creating a gaming system with appropriate CPU and GPU balance, a workstation with sufficient memory and storage, or a compact system with appropriate cooling solutions. Examples should include designing a balanced gaming system that avoids bottlenecks between CPU and GPU, or creating a quiet home theater PC with appropriate cooling solutions.

**5.5.10 Explain compatibility considerations in system building**
- Identify critical compatibility factors in system building (CPU-motherboard, RAM-motherboard, GPU-case)
- Explain how to research and verify component compatibility
- Understand the concept of bottlenecks and how to avoid them
- Apply compatibility knowledge to troubleshoot system integration issues

**Guidance:** Students should be able to identify and explain the critical compatibility factors that must be considered when building a computer system. They should understand CPU-motherboard compatibility (socket type, chipset support, BIOS updates), RAM-motherboard compatibility (memory type, speed, capacity limits), GPU-case compatibility (physical length, width, power requirements), and power supply compatibility (wattage, connector types, efficiency ratings). Students should explain how to research and verify component compatibility using manufacturer specifications, compatibility lists, online resources, and community forums. They should understand the concept of bottlenecks, where one component significantly limits the performance of other components (e.g., a low-end CPU limiting a high-end GPU, or insufficient RAM causing system slowdowns), and learn strategies to avoid them through balanced component selection. Students should apply this knowledge to troubleshoot system integration issues, identifying compatibility problems that might prevent a system from functioning properly or performing optimally. Examples should include diagnosing why a system won't boot (incompatible RAM) or why gaming performance is poor (CPU bottlenecking a powerful GPU).

**5.5.11 Explain system configuration and optimization**
- Explain the role of BIOS/UEFI settings in system configuration
- Identify key system settings that affect performance (memory timings, power management)
- Understand the principles of system optimization for different use cases
- Apply configuration knowledge to optimize systems for specific applications

**Guidance:** Students should understand the role of BIOS/UEFI settings in configuring system hardware and optimizing performance. They should identify key system settings that affect performance, including memory timings (CAS latency, RAS to CAS delay, etc.), memory frequency, power management settings (CPU power states, fan curves), virtualization support, and boot order configuration. Students should understand the principles of system optimization for different use cases, such as prioritizing memory performance for content creation, optimizing cooling for gaming systems, or maximizing energy efficiency for always-on systems. They should learn about the trade-offs involved in optimization, such as performance versus power consumption, stability versus overclocking, or noise versus cooling efficiency. Students should apply this knowledge to optimize systems for specific applications, such as configuring memory settings for maximum bandwidth in a workstation, adjusting fan curves for optimal cooling in a gaming system, or enabling virtualization support for a system that will run virtual machines. Examples should include optimizing a gaming system for maximum frame rates, configuring a workstation for stable content creation, or setting up a home server for energy efficiency.

**5.5.12 Explain troubleshooting motherboard and integration issues**
- Identify common motherboard-related issues (no boot, instability, component recognition)
- Explain systematic approaches to diagnosing hardware problems
- Understand the use of diagnostic tools and methods (POST codes, beep codes, diagnostic software)
- Apply troubleshooting knowledge to resolve common system integration problems

**Guidance:** Students should be able to identify common motherboard-related issues that can affect system operation, including failure to boot (no power, no display), system instability (random crashes, blue screens), and component recognition problems (RAM not detected, storage devices missing). They should explain systematic approaches to diagnosing hardware problems, including the process of elimination (removing or replacing components one at a time), minimal boot testing (running the system with only essential components), and careful observation of symptoms and error messages. Students should understand the use of diagnostic tools and methods, including POST (Power-On Self-Test) codes displayed on motherboards with debug LEDs or segment displays, beep codes for motherboards with speakers, and diagnostic software that tests hardware components and reports issues. They should apply this knowledge to resolve common system integration problems, such as diagnosing why a system won't boot (checking power supply connections, RAM seating, CPU installation), troubleshooting stability issues (testing memory, checking temperatures, updating drivers), or resolving component recognition problems (checking connections, updating BIOS/UEFI). Examples should include step-by-step troubleshooting of a system that fails to boot, a system that crashes under load, or a storage device that isn't detected by the system.



# Topic 6: Input/Output Devices and Peripherals

Students will be assessed on their ability to:

**6.6.1 Define input devices and their role in computer systems**
- Define input devices as hardware components that allow users to enter data and instructions into a computer system
- Explain the fundamental role of input devices in the human-computer interaction cycle
- Classify input devices based on their functionality and method of operation
- Understand the relationship between input devices and system responsiveness

**Guidance:** Students should understand input devices as hardware components that convert user actions or environmental data into signals that a computer can process. They should explain that input devices serve as the primary interface between users and computers, enabling data entry, command execution, and system control. Students should be able to classify input devices into categories such as text input (keyboards), pointing devices (mice, trackpads), audio input (microphones), image capture (cameras, scanners), and specialized input (game controllers, biometric readers). They should understand how the quality and capabilities of input devices directly impact system responsiveness and user experience, with factors like input latency, precision, and ergonomics playing important roles. Examples should include how a gaming mouse with higher DPI (dots per inch) provides more precise cursor control, or how a mechanical keyboard with faster response times improves typing speed and accuracy.

**6.6.2 Explain the operation and characteristics of keyboards**
- Identify the main types of keyboards (mechanical, membrane, scissor-switch, capacitive)
- Explain the technology behind different keyboard mechanisms
- Understand keyboard layouts and their variations (QWERTY, AZERTY, DVORAK)
- Analyze keyboard features that affect usability and performance (key rollover, anti-ghosting, backlighting)

**Guidance:** Students should be able to identify and explain different types of keyboards based on their underlying technology. They should describe mechanical keyboards as using individual physical switches for each key, providing tactile feedback and durability; membrane keyboards as using rubber dome switches under a flexible membrane; scissor-switch keyboards as using a scissor-like mechanism for low-profile keys; and capacitive keyboards as detecting key presses through changes in capacitance. Students should understand different keyboard layouts beyond the standard QWERTY, including AZERTY (used in French-speaking countries), DVORAK (designed for typing efficiency), and specialized layouts for programming or gaming. They should analyze keyboard features that affect performance, such as key rollover (the ability to register multiple simultaneous key presses), anti-ghosting (preventing unintended key presses), and backlighting (improving visibility in low-light conditions). Examples should include selecting appropriate keyboards for different scenarios, such as mechanical keyboards for gaming and typing enthusiasts, or low-profile keyboards for portable devices.

**6.6.3 Explain the operation and characteristics of pointing devices**
- Identify different types of pointing devices (mouse, trackpad, trackball, pointing stick, touch screen)
- Explain the technology behind various pointing mechanisms (optical, laser, touch, capacitive)
- Understand how pointing devices translate physical movement into cursor control
- Analyze pointing device specifications that affect performance (DPI, polling rate, acceleration)

**Guidance:** Students should be able to identify and explain various types of pointing devices used for cursor control and navigation. They should describe mice as handheld devices with one or more buttons that detect movement relative to a surface; trackpads as flat surfaces that detect finger movement; trackballs as stationary devices with a rotating ball controlled by fingers; pointing sticks as small joysticks typically found in the middle of keyboards; and touch screens as displays that detect direct finger or stylus contact. Students should explain the technology behind these devices, including optical mice (using LED light and a camera to track movement), laser mice (using laser light for higher precision), capacitive touch screens (detecting changes in electrical fields), and resistive touch screens (detecting pressure). They should understand how these devices translate physical movement into cursor control through sensors that detect motion, convert it to digital signals, and communicate these signals to the computer. Students should analyze specifications that affect performance, such as DPI (dots per inch, measuring sensitivity), polling rate (how often the device reports its position to the computer), and acceleration (how cursor speed changes with movement speed). Examples should include selecting appropriate pointing devices for different applications, such as high-DPI mice for graphic design or gaming, or precision trackballs for detailed work.

**6.6.4 Explain the operation and characteristics of audio input devices**
- Identify different types of audio input devices (microphones, headsets, voice recorders)
- Explain the technology behind audio capture (dynamic, condenser, ribbon microphones)
- Understand audio input specifications and their impact on quality (frequency response, sensitivity, bit depth)
- Analyze the use of audio input in various applications (voice recognition, communication, recording)

**Guidance:** Students should be able to identify and explain different types of audio input devices used to capture sound. They should describe microphones as devices that convert sound waves into electrical signals, headsets as combinations of headphones and microphones for communication, and voice recorders as dedicated devices for capturing audio. Students should explain the technology behind different microphone types, including dynamic microphones (using electromagnetic induction to capture sound), condenser microphones (using a capacitor to convert sound to electrical signals), and ribbon microphones (using a thin metal ribbon suspended in a magnetic field). They should understand audio input specifications that affect quality, such as frequency response (the range of frequencies a microphone can capture), sensitivity (how effectively a microphone converts sound pressure to electrical voltage), and bit depth (the number of bits used to represent each audio sample). Students should analyze how these devices are used in various applications, including voice recognition systems (requiring clear audio input), communication applications (needing noise cancellation), and professional audio recording (requiring high fidelity). Examples should include selecting appropriate microphones for different scenarios, such as condenser microphones for studio recording or directional microphones for reducing background noise in voice commands.

**6.6.5 Explain the operation and characteristics of image and video input devices**
- Identify different types of image and video input devices (digital cameras, webcams, scanners, document cameras)
- Explain the technology behind image capture (CCD, CMOS sensors, lens systems)
- Understand image and video specifications and their impact on quality (resolution, frame rate, dynamic range)
- Analyze the use of image and video input in various applications (video conferencing, document scanning, security)

**Guidance:** Students should be able to identify and explain different types of devices used to capture images and videos. They should describe digital cameras as dedicated devices for capturing high-quality still images and videos, webcams as cameras designed for video communication, scanners as devices for digitizing printed documents or photos, and document cameras as real-time image capture devices for presenting physical documents. Students should explain the technology behind image capture, including CCD (Charge-Coupled Device) sensors that convert light into electrical signals, CMOS (Complementary Metal-Oxide-Semiconductor) sensors that perform the same function with lower power consumption, and lens systems that focus light onto the sensors. They should understand image and video specifications that affect quality, such as resolution (number of pixels in an image), frame rate (number of frames captured per second in video), and dynamic range (the ability to capture details in both bright and dark areas). Students should analyze how these devices are used in various applications, including video conferencing (requiring adequate resolution and frame rate), document scanning (requiring accurate color reproduction), and security systems (requiring motion detection and low-light performance). Examples should include selecting appropriate cameras for different purposes, such as high-resolution scanners for document archiving or low-light cameras for security applications.

**6.6.6 Define output devices and their role in computer systems**
- Define output devices as hardware components that present or display information processed by a computer
- Explain the fundamental role of output devices in completing the human-computer interaction cycle
- Classify output devices based on their functionality and method of operation
- Understand the relationship between output devices and system usability

**Guidance:** Students should understand output devices as hardware components that convert digital information from a computer into forms perceivable by humans. They should explain that output devices complete the human-computer interaction cycle by presenting processed data, results, and feedback to users. Students should be able to classify output devices into categories such as visual display (monitors, projectors), audio output (speakers, headphones), print output (printers, plotters), and specialized output (haptic feedback devices, Braille displays). They should understand how the quality and capabilities of output devices directly impact system usability and user experience, with factors like display resolution, audio fidelity, print quality, and response time playing important roles. Examples should include how a high-resolution monitor improves productivity by displaying more information simultaneously, or how high-quality headphones enhance the multimedia experience by providing accurate audio reproduction.

**6.6.7 Explain the operation and characteristics of display devices**
- Identify different types of display devices (LCD, LED, OLED, CRT, projection systems)
- Explain the technology behind various display types (liquid crystals, organic light-emitting diodes)
- Understand display specifications and their impact on visual quality (resolution, refresh rate, color gamut)
- Analyze the suitability of different display types for various applications (gaming, professional work, general use)

**Guidance:** Students should be able to identify and explain different types of display technologies used in computer monitors and other visual output devices. They should describe LCD (Liquid Crystal Display) as using liquid crystals that twist to control light passage; LED (Light Emitting Diode) displays as using LEDs for backlighting in LCD panels or as individual pixels in LED displays; OLED (Organic Light Emitting Diode) as using organic compounds that emit light when electricity is applied; CRT (Cathode Ray Tube) as using electron beams to illuminate phosphors on a glass screen; and projection systems as projecting images onto a surface using light sources and lenses. Students should explain the underlying technology of these displays, including how LCD pixels work with polarized light and color filters, how OLED pixels emit their own light, and how projection systems create images using DLP (Digital Light Processing) or LCD technology. They should understand display specifications that affect visual quality, such as resolution (number of pixels), refresh rate (how often the image is updated per second), and color gamut (the range of colors a display can reproduce). Students should analyze the suitability of different display types for various applications, such as high-refresh-rate monitors for gaming, color-accurate displays for professional photo editing, or OLED displays for high-contrast media consumption.

**6.6.8 Explain the operation and characteristics of audio output devices**
- Identify different types of audio output devices (speakers, headphones, earbuds, sound systems)
- Explain the technology behind audio reproduction (dynamic drivers, balanced armature, planar magnetic)
- Understand audio output specifications and their impact on sound quality (frequency response, impedance, total harmonic distortion)
- Analyze the use of audio output in various applications (music production, gaming, communication)

**Guidance:** Students should be able to identify and explain different types of devices used to reproduce audio from computers. They should describe speakers as devices that convert electrical signals into audible sound for open listening environments; headphones as devices worn over or in the ears for personal listening; earbuds as small headphones that fit inside the ear canal; and sound systems as combinations of multiple speakers and audio components for immersive audio experiences. Students should explain the technology behind audio reproduction, including dynamic drivers (using a diaphragm attached to a voice coil within a magnetic field), balanced armature drivers (using a balanced armature that moves in response to electrical signals), and planar magnetic drivers (using a thin diaphragm with a conductive trace placed between magnets). They should understand audio output specifications that affect sound quality, such as frequency response (the range of frequencies a device can reproduce), impedance (electrical resistance affecting power requirements), and total harmonic distortion (a measure of signal purity). Students should analyze how these devices are used in various applications, including music production (requiring accurate sound reproduction), gaming (requiring positional audio), and communication (requiring clear voice reproduction). Examples should include selecting appropriate audio devices for different scenarios, such as studio monitors for audio production or gaming headsets with surround sound for immersive gaming experiences.

**6.6.9 Explain the operation and characteristics of print output devices**
- Identify different types of print output devices (inkjet, laser, dot matrix, 3D printers)
- Explain the technology behind various printing methods (thermal, laser, impact, additive manufacturing)
- Understand print specifications and their impact on output quality (resolution, color depth, print speed)
- Analyze the suitability of different printing technologies for various applications (document printing, photo printing, prototyping)

**Guidance:** Students should be able to identify and explain different types of devices used to produce physical copies of digital information. They should describe inkjet printers as devices that spray droplets of ink onto paper; laser printers as devices that use toner powder fused to paper with heat; dot matrix printers as devices that strike ink-soaked ribbons against paper; and 3D printers as devices that create physical objects by adding material layer by layer. Students should explain the technology behind these printing methods, including thermal inkjet (using heat to create bubbles that force ink droplets out of nozzles), laser printing (using a laser beam to create an electrostatic image that attracts toner particles), impact printing (using pins striking an inked ribbon), and additive manufacturing (building objects layer by layer from digital models). They should understand print specifications that affect output quality, such as resolution (measured in dots per inch), color depth (number of colors that can be reproduced), and print speed (pages per minute). Students should analyze the suitability of different printing technologies for various applications, such as laser printers for high-volume document printing, inkjet printers for photo printing, and 3D printers for rapid prototyping and manufacturing. Examples should include selecting appropriate printers for different scenarios, such as high-resolution photo printers for photography or industrial 3D printers for manufacturing applications.

**6.6.10 Explain the operation and characteristics of specialized output devices**
- Identify different types of specialized output devices (haptic feedback devices, Braille displays, data projectors, plotters)
- Explain the technology behind specialized output methods (force feedback, tactile stimulation, large-format printing)
- Understand the applications and use cases for specialized output devices
- Analyze how specialized output devices enhance accessibility and functionality for specific user groups

**Guidance:** Students should be able to identify and explain various types of specialized output devices designed for specific applications or user needs. They should describe haptic feedback devices as systems that provide tactile sensations to users through vibrations, forces, or motions; Braille displays as devices that create tactile Braille characters for visually impaired users; data projectors as devices that display computer output on large screens or surfaces; and plotters as devices that produce large-format technical drawings and graphics. Students should explain the technology behind these specialized output methods, including force feedback (using motors and actuators to create resistance and motion), tactile stimulation (using pins or other mechanisms to create touchable surfaces), and large-format printing (using specialized mechanisms to handle wide paper or other media). They should understand the specific applications and use cases for these devices, such as haptic feedback in gaming controllers and medical simulators, Braille displays for accessibility, data projectors for presentations and education, and plotters for architectural and engineering drawings. Students should analyze how these specialized output devices enhance accessibility and functionality for specific user groups, such as how Braille displays enable visually impaired users to access digital text, or how haptic feedback systems improve training in medical and industrial applications. Examples should include selecting appropriate specialized output devices for specific scenarios, such as force feedback joysticks for flight simulation or wide-format plotters for architectural design.

**6.6.11 Define input/output interfaces and their role in computer systems**
- Define I/O interfaces as standardized methods for connecting peripherals to computers
- Explain the fundamental role of interfaces in facilitating communication between devices and systems
- Classify interfaces based on their characteristics (wired, wireless, serial, parallel)
- Understand the relationship between interface standards and device compatibility

**Guidance:** Students should understand I/O interfaces as standardized hardware and software protocols that enable communication between peripheral devices and computer systems. They should explain that interfaces serve as the bridge between devices and computers, translating signals and data into formats that can be understood by both parties. Students should be able to classify interfaces based on their characteristics, including wired interfaces (using physical cables), wireless interfaces (using radio frequencies or infrared), serial interfaces (transmitting data one bit at a time), and parallel interfaces (transmitting multiple bits simultaneously). They should understand how interface standards ensure compatibility between devices from different manufacturers, allowing users to connect various peripherals to their systems. Examples should include how USB standards enable connection of thousands of different devices to computers, or how wireless standards like Bluetooth allow for cable-free connections between devices.

**6.6.12 Explain the operation and characteristics of wired I/O interfaces**
- Identify common wired I/O interfaces (USB, HDMI, DisplayPort, Thunderbolt, Ethernet)
- Explain the technology behind various wired connection standards
- Understand interface specifications and their impact on performance (bandwidth, version, connector types)
- Analyze the suitability of different wired interfaces for various applications (data transfer, video output, networking)

**Guidance:** Students should be able to identify and explain common wired interfaces used to connect peripherals to computers. They should describe USB (Universal Serial Bus) as a universal standard for connecting a wide variety of devices; HDMI (High-Definition Multimedia Interface) as a standard for transmitting video and audio; DisplayPort as a digital display interface primarily for connecting monitors; Thunderbolt as a high-speed interface that combines data transfer, video output, and power delivery; and Ethernet as a standard for wired network connections. Students should explain the technology behind these interfaces, including how USB uses differential signaling for data transmission, how HDMI and DisplayPort encode video and audio data, how Thunderbolt combines PCIe and DisplayPort protocols, and how Ethernet uses packet switching for network communication. They should understand interface specifications that affect performance, such as bandwidth (data transfer capacity), version (indicating improvements and capabilities over time), and connector types (physical form factors). Students should analyze the suitability of different wired interfaces for various applications, such as using USB for general peripheral connectivity, HDMI or DisplayPort for video output, Thunderbolt for high-speed data transfer and docking stations, and Ethernet for reliable network connections. Examples should include selecting appropriate interfaces for different scenarios, such as USB-C with Thunderbolt for connecting multiple high-speed devices or DisplayPort for high-refresh-rate gaming monitors.

**6.6.13 Explain the operation and characteristics of wireless I/O interfaces**
- Identify common wireless I/O interfaces (Bluetooth, Wi-Fi, NFC, IR)
- Explain the technology behind various wireless connection standards
- Understand wireless interface specifications and their impact on performance (range, bandwidth, power consumption)
- Analyze the suitability of different wireless interfaces for various applications (short-range connectivity, networking, contactless interactions)

**Guidance:** Students should be able to identify and explain common wireless interfaces used to connect peripherals to computers without physical cables. They should describe Bluetooth as a short-range wireless technology for connecting peripherals; Wi-Fi as a wireless networking technology for local area network connections; NFC (Near Field Communication) as a very short-range technology for contactless interactions; and IR (Infrared) as a line-of-sight wireless technology using infrared light. Students should explain the technology behind these interfaces, including how Bluetooth uses frequency-hopping spread spectrum in the 2.4 GHz band, how Wi-Fi uses radio waves based on IEEE 802.11 standards, how NFC uses electromagnetic induction between closely positioned coils, and how IR uses pulses of infrared light to transmit data. They should understand wireless interface specifications that affect performance, such as range (maximum distance for reliable connection), bandwidth (data transfer capacity), and power consumption (battery impact). Students should analyze the suitability of different wireless interfaces for various applications, such as Bluetooth for connecting mice, keyboards, and headphones; Wi-Fi for network connectivity and internet access; NFC for contactless payments and data exchange; and IR for remote control applications. Examples should include selecting appropriate wireless interfaces for different scenarios, such as Bluetooth Low Energy for battery-powered peripherals or Wi-Fi 6 for high-bandwidth applications like video streaming.

**6.6.14 Explain the principles of human-computer interaction (HCI)**
- Define human-computer interaction as the study of how people interact with computers
- Explain the fundamental goals of HCI (usability, efficiency, accessibility)
- Identify key concepts in HCI (user experience, user interface, interaction design)
- Understand the relationship between HCI principles and effective system design

**Guidance:** Students should understand human-computer interaction as a multidisciplinary field focused on the design, evaluation, and implementation of interactive computing systems for human use. They should explain that the fundamental goals of HCI include creating systems that are usable (easy to learn and use), efficient (allowing users to accomplish tasks quickly), and accessible (usable by people with diverse abilities and disabilities). Students should identify key concepts in HCI, including user experience (the overall experience a person has when using a system), user interface (the means by which a user interacts with a system), and interaction design (the process of creating interfaces with well-thought-out behaviors). They should understand how HCI principles guide the development of effective systems that meet user needs, minimize errors, and enhance productivity. Examples should include how applying HCI principles can lead to more intuitive software interfaces, more ergonomic hardware designs, and more accessible systems for users with disabilities.

**6.6.15 Explain the evolution of human-computer interfaces**
- Identify major milestones in the evolution of computer interfaces (command-line, graphical, touch, voice, gesture)
- Explain the technological advancements that enabled each interface paradigm
- Understand the relationship between interface evolution and increasing accessibility
- Analyze how different interface paradigms suit different user needs and contexts

**Guidance:** Students should be able to identify and explain the major milestones in the evolution of how humans interact with computers. They should describe the progression from early command-line interfaces (requiring users to type text commands) to graphical user interfaces (using windows, icons, menus, and pointers), touch interfaces (direct manipulation with fingers), voice interfaces (using spoken commands), and gesture interfaces (using body movements and motions). Students should explain the technological advancements that enabled each interface paradigm, including the development of display technology for GUIs, touch-sensitive screens for touch interfaces, speech recognition algorithms for voice interfaces, and motion sensors for gesture interfaces. They should understand how this evolution has generally increased accessibility, making computers usable by people with less technical knowledge and diverse abilities. Students should analyze how different interface paradigms suit different user needs and contexts, such as command-line interfaces for precise control and automation, graphical interfaces for general usability, touch interfaces for mobile devices, voice interfaces for hands-free operation, and gesture interfaces for immersive experiences. Examples should include selecting appropriate interface paradigms for different scenarios, such as voice interfaces for smart home devices or gesture interfaces for virtual reality systems.

**6.6.16 Explain the principles of user interface design**
- Identify key principles of effective user interface design (consistency, feedback, simplicity, flexibility)
- Explain how these principles contribute to usable interfaces
- Understand the relationship between interface design and user performance
- Apply interface design principles to evaluate existing interfaces

**Guidance:** Students should be able to identify and explain key principles that guide the design of effective user interfaces. They should describe principles such as consistency (maintaining uniform design elements and behaviors throughout an interface), feedback (providing clear responses to user actions), simplicity (minimizing complexity and avoiding unnecessary elements), and flexibility (accommodating different user preferences and needs). Students should explain how these principles contribute to usable interfaces by reducing learning time, minimizing errors, increasing efficiency, and improving user satisfaction. They should understand the relationship between interface design and user performance, with well-designed interfaces enabling users to complete tasks more quickly, accurately, and with less cognitive load. Students should apply these principles to evaluate existing interfaces, identifying strengths and weaknesses based on how well they adhere to established design principles. Examples should include evaluating software applications, websites, or hardware interfaces based on these design principles, suggesting improvements where deficiencies are identified.

**6.6.17 Explain accessibility considerations in human-computer interaction**
- Define accessibility in the context of human-computer interaction
- Identify different types of disabilities and how they affect computer use (visual, auditory, motor, cognitive)
- Explain assistive technologies and interface adaptations that improve accessibility
- Apply accessibility principles to design inclusive computing experiences

**Guidance:** Students should understand accessibility in HCI as the design of computer systems that can be used by people with diverse abilities, including those with disabilities. They should identify different types of disabilities and how they affect computer use, including visual impairments (affecting ability to see screen content), auditory impairments (affecting ability to hear audio cues), motor impairments (affecting ability to use input devices), and cognitive impairments (affecting ability to understand complex interfaces). Students should explain assistive technologies and interface adaptations that improve accessibility, such as screen readers (converting text to speech for visually impaired users), captioning (providing text alternatives to audio), alternative input devices (accommodating limited motor control), and simplified interfaces (reducing complexity for users with cognitive disabilities). They should apply accessibility principles to design inclusive computing experiences that can be used by the widest possible range of users, regardless of ability. Examples should include designing websites that work with screen readers, creating software with customizable interface options, or developing hardware with multiple input methods to accommodate different user needs.

**6.6.18 Explain emerging trends in human-computer interaction**
- Identify emerging technologies and approaches in HCI (virtual reality, augmented reality, brain-computer interfaces)
- Explain how these technologies are changing how humans interact with computers
- Understand the potential impact of emerging HCI technologies on various fields
- Analyze the challenges and opportunities presented by new interaction paradigms

**Guidance:** Students should be able to identify and explain emerging technologies and approaches that are shaping the future of human-computer interaction. They should describe virtual reality (VR) as creating fully immersive digital environments, augmented reality (AR) as overlaying digital information onto the physical world, and brain-computer interfaces (BCIs) as enabling direct communication between the brain and computers. Students should explain how these technologies are changing human-computer interaction by creating more natural, immersive, and direct ways of controlling and experiencing digital content. They should understand the potential impact of these technologies across various fields, including education (immersive learning experiences), healthcare (surgical training and rehabilitation), entertainment (new forms of media and gaming), and accessibility (new ways for people with disabilities to interact with technology). Students should analyze the challenges and opportunities presented by these new interaction paradigms, such as technical limitations, ethical considerations, health and safety concerns, and the potential for more intuitive and accessible computing experiences. Examples should include evaluating current VR/AR applications, considering the potential future applications of BCIs, or discussing how these technologies might address current limitations in human-computer interaction.



# Topic 7: Hardware for AI and Machine Learning

Students will be assessed on their ability to:

**7.7.1 Define AI/ML workloads and their computational characteristics**
- Define artificial intelligence (AI) and machine learning (ML) workloads as computational tasks that involve pattern recognition, data analysis, and model training
- Explain the computational characteristics of AI/ML workloads (parallel processing, high memory bandwidth, floating-point operations)
- Identify different types of AI/ML workloads (training, inference, reinforcement learning)
- Understand how AI/ML workloads differ from traditional computing tasks

**Guidance:** Students should understand AI/ML workloads as computational tasks focused on creating or using models that can recognize patterns, make predictions, or decisions based on data. They should explain that these workloads are characterized by massive parallel processing requirements (performing many calculations simultaneously), high memory bandwidth needs (moving large amounts of data quickly), and intensive floating-point operations (mathematical calculations with decimal numbers). Students should identify different types of AI/ML workloads, including training (creating models by processing large datasets), inference (using trained models to make predictions), and reinforcement learning (training models through trial and error). They should understand how AI/ML workloads differ from traditional computing tasks in their emphasis on parallel rather than sequential processing, their tolerance for approximate results rather than precise calculations, and their need for specialized hardware acceleration rather than general-purpose processing. Examples should include comparing how a word processor uses a CPU differently from how a neural network training process uses a GPU.

**7.7.2 Explain the relationship between algorithm complexity and hardware requirements**
- Define algorithm complexity in the context of AI/ML
- Explain how different AI/ML algorithms have varying computational requirements
- Identify factors that affect algorithm complexity (model size, data volume, precision requirements)
- Understand how algorithm complexity translates to specific hardware needs

**Guidance:** Students should understand algorithm complexity as a measure of the computational resources required to execute an AI/ML algorithm, typically expressed in terms of time complexity (how execution time grows with input size) and space complexity (how memory usage grows with input size). They should explain how different AI/ML algorithms have varying computational requirements, such as deep neural networks requiring significantly more resources than simpler machine learning algorithms like decision trees or linear regression. Students should identify factors that affect algorithm complexity, including model size (number of parameters in a neural network), data volume (size of training datasets), and precision requirements (using 32-bit floating-point versus 8-bit integer calculations). They should understand how these factors translate to specific hardware needs, such as how larger models require more GPU memory, how larger datasets require faster storage and more RAM, and how lower precision requirements can enable better performance on specialized hardware. Examples should include comparing the hardware requirements for training a small image classification model versus a large language model.

**7.7.3 Identify key hardware components for AI systems and their roles**
- Identify the CPU and its role in AI systems (data preprocessing, orchestration)
- Identify the GPU and its role in AI systems (parallel processing, acceleration)
- Identify memory (RAM) and its role in AI systems (holding datasets, intermediate results)
- Identify storage and its role in AI systems (storing datasets, models, checkpoints)
- Identify specialized AI accelerators and their role (TPUs, NPUs, FPGAs)

**Guidance:** Students should be able to identify and explain the role of each key hardware component in AI systems. They should understand that while CPUs handle general-purpose computing tasks in AI systems, including data preprocessing, task orchestration, and running parts of algorithms that don't parallelize well. Students should identify GPUs as the primary workhorses for most AI systems, handling the parallel processing required for neural network training and inference through their thousands of cores optimized for matrix operations. They should explain that RAM serves as the working memory that holds datasets, model parameters, and intermediate results during processing, with insufficient RAM leading to significant performance bottlenecks. Students should identify storage systems as critical for storing large datasets, trained models, and training checkpoints, with faster storage (SSDs) reducing data loading times. They should also identify specialized AI accelerators like TPUs (Tensor Processing Units), NPUs (Neural Processing Units), and FPGAs (Field-Programmable Gate Arrays) as hardware specifically designed to accelerate AI workloads with optimized architectures for common AI operations. Examples should include how each component contributes to the overall AI system performance.

**7.7.4 Understand memory requirements for AI/ML workloads**
- Explain the memory demands of different AI/ML tasks
- Identify factors affecting memory requirements (model size, batch size, data precision)
- Understand the relationship between memory capacity and AI performance
- Apply memory requirements knowledge to configure AI systems

**Guidance:** Students should understand that AI/ML workloads often have substantial memory requirements that can significantly impact system performance and capability. They should explain how different AI/ML tasks have varying memory demands, with large language models and computer vision models typically requiring more memory than simpler classification tasks. Students should identify factors affecting memory requirements, including model size (number of parameters that need to be stored in memory), batch size (number of samples processed simultaneously), and data precision (bit depth of numerical representations, such as 32-bit float, 16-bit float, or 8-bit integer). They should understand the relationship between memory capacity and AI performance, including how insufficient memory forces systems to use slower storage as virtual memory, dramatically reducing performance, and how adequate memory allows for larger batch sizes and more complex models. Students should apply this knowledge to configure AI systems by selecting appropriate memory capacity based on intended workloads, such as choosing 16GB of RAM for basic AI tasks versus 64GB or more for training large language models. Examples should include calculating memory requirements for specific models and batch sizes.

**7.7.5 Explain storage requirements for AI/ML systems**
- Explain the storage demands of AI/ML datasets and models
- Identify factors affecting storage requirements (dataset size, model checkpoints, intermediate results)
- Understand the relationship between storage speed and AI performance
- Apply storage requirements knowledge to design AI systems

**Guidance:** Students should understand that AI/ML systems often require substantial storage capacity and performance to handle large datasets, model files, and intermediate results. They should explain how AI/ML datasets can range from gigabytes to petabytes in size, with models themselves also consuming significant storage space, especially when multiple versions or checkpoints are saved during training. Students should identify factors affecting storage requirements, including dataset size (volume of training data), model checkpoints (saved states during training), and intermediate results (temporary data generated during processing). They should understand the relationship between storage speed and AI performance, including how faster storage (SSDs versus HDDs) reduces data loading times, which can be a significant bottleneck in training pipelines, especially for data-intensive tasks. Students should apply this knowledge to design AI systems by selecting appropriate storage solutions based on capacity needs (terabytes for large datasets) and performance requirements (NVMe SSDs for high-throughput scenarios). Examples should include designing storage systems for different AI scenarios, from small-scale development to large-scale training environments.

**7.7.6 Analyze the computational demands of different AI/ML algorithms**
- Identify computational characteristics of different AI/ML algorithm types
- Explain how neural network architectures affect computational requirements
- Understand the relationship between model accuracy and computational cost
- Apply computational analysis to select appropriate hardware for specific algorithms

**Guidance:** Students should be able to analyze and compare the computational demands of different AI/ML algorithms. They should identify the computational characteristics of various algorithm types, including deep learning (highly parallel, intensive matrix operations), traditional machine learning (less parallel, more diverse operations), and reinforcement learning (frequent model updates, simulation requirements). Students should explain how neural network architectures affect computational requirements, including how factors like network depth (number of layers), width (number of neurons per layer), and connectivity patterns (dense versus sparse connections) impact processing needs. They should understand the relationship between model accuracy and computational cost, recognizing that more accurate models often require significantly more computational resources, creating trade-offs between performance and resource efficiency. Students should apply this computational analysis to select appropriate hardware for specific algorithms, such as choosing high-end GPUs for large transformer models versus more modest hardware for smaller convolutional networks. Examples should include comparing the computational requirements of different neural network architectures and their implications for hardware selection.

**7.7.7 Apply hardware knowledge to evaluate system suitability for AI tasks**
- Explain how to assess whether a system can handle specific AI workloads
- Identify key performance indicators for AI systems (FLOPS, memory bandwidth, latency)
- Understand benchmarking approaches for AI hardware evaluation
- Apply evaluation methodologies to determine system capabilities

**Guidance:** Students should understand how to evaluate whether a computer system is suitable for specific AI tasks based on its hardware specifications. They should explain the process of assessing system capabilities by examining key components (CPU, GPU, memory, storage) and comparing them against the requirements of intended AI workloads. Students should identify key performance indicators for AI systems, including FLOPS (Floating Point Operations Per Second, measuring raw computational power), memory bandwidth (data transfer rate between memory and processors), and latency (response time for operations). They should understand benchmarking approaches for AI hardware evaluation, including standard benchmarks like MLPerf that measure performance across various AI tasks, and custom benchmarks that simulate specific workloads. Students should apply these evaluation methodologies to determine system capabilities by running benchmark tests, analyzing performance metrics, and identifying potential bottlenecks. Examples should include evaluating different systems for specific AI applications like image recognition, natural language processing, or recommendation systems.

**7.7.8 Identify AMD's AI hardware ecosystem and product lines**
- Define AMD's approach to AI hardware as a comprehensive ecosystem
- Identify AMD's product lines for AI computing (Ryzen AI, Radeon, EPYC, adaptive SoCs)
- Explain AMD's strategy of heterogeneous computing for AI workloads
- Understand how AMD's AI hardware integrates with broader system architectures

**Guidance:** Students should understand AMD's approach to AI hardware as a comprehensive ecosystem that spans multiple product categories and computing paradigms. They should identify AMD's key product lines for AI computing, including Ryzen AI processors (consumer CPUs with integrated AI acceleration), Radeon GPUs (graphics processors optimized for AI workloads), EPYC processors (server CPUs for data center AI), and adaptive SoCs (System on Chip products with specialized AI engines). Students should explain AMD's strategy of heterogeneous computing for AI workloads, which involves using different types of processors (CPUs, GPUs, specialized accelerators) together in a coordinated fashion to optimize performance and efficiency. They should understand how AMD's AI hardware integrates with broader system architectures through technologies like Infinity Fabric (high-speed interconnect) and open software standards that enable seamless communication between components. Examples should include how AMD's ecosystem allows for flexible configurations ranging from consumer laptops with Ryzen AI to data center deployments with EPYC processors and Radeon GPUs.

**7.7.9 Explain AMD's approach to AI computing architecture**
- Identify the key principles of AMD's AI computing architecture
- Explain AMD's focus on open-source software for AI development
- Understand AMD's approach to heterogeneous computing for AI
- Analyze how AMD's architectural choices impact AI performance and efficiency

**Guidance:** Students should be able to explain the fundamental principles that guide AMD's approach to AI computing architecture. They should identify key principles including performance-per-watt optimization (maximizing computational efficiency), scalability (supporting configurations from small devices to large data centers), and open standards (promoting interoperability and community development). Students should explain AMD's focus on open-source software for AI development, including ROCm (Radeon Open Compute platform) as an alternative to NVIDIA's CUDA, and support for popular frameworks like TensorFlow and PyTorch through open-source libraries. They should understand AMD's approach to heterogeneous computing, which involves designing architectures that can efficiently distribute AI workloads across different types of processors based on their strengths, such as using CPUs for sequential tasks, GPUs for parallel processing, and specialized accelerators for specific operations. Students should analyze how these architectural choices impact AI performance and efficiency, including how open-source approaches enable broader adoption and how heterogeneous computing can improve overall system efficiency. Examples should include comparing AMD's architectural approach with competitors' strategies.

**7.7.10 Understand AMD Ryzen AI processors and their capabilities**
- Define AMD Ryzen AI as a processor line with integrated AI acceleration
- Identify the components of Ryzen AI processors (CPU cores, GPU cores, AI Engine)
- Explain the capabilities and performance characteristics of Ryzen AI
- Understand the target applications and use cases for Ryzen AI processors

**Guidance:** Students should understand AMD Ryzen AI as a line of consumer processors that integrate specialized AI acceleration capabilities directly into the CPU. They should identify the key components of Ryzen AI processors, including traditional CPU cores for general-purpose computing, integrated GPU cores for graphics and parallel processing, and the AI Engine (a dedicated neural processing unit designed specifically for AI workloads). Students should explain the capabilities and performance characteristics of Ryzen AI, including its ability to accelerate common AI tasks like image recognition, noise reduction, and background blur while maintaining low power consumption suitable for mobile devices. They should understand the target applications and use cases for Ryzen AI processors, which include consumer laptops and all-in-one systems that benefit from on-device AI processing for improved privacy, reduced latency, and lower cloud dependency. Examples should include specific applications like video conferencing with background blur, photo enhancement, and voice recognition that can be accelerated by Ryzen AI processors.

**7.7.11 Explain AMD Radeon GPUs for AI workloads**
- Define AMD Radeon GPUs as graphics processors optimized for AI computing
- Identify the architectural features of Radeon GPUs that benefit AI workloads
- Explain the performance characteristics of Radeon GPUs for AI training and inference
- Understand how Radeon GPUs compare to competing solutions for AI applications

**Guidance:** Students should understand AMD Radeon GPUs as graphics processing units specifically designed with features and optimizations for AI computing workloads. They should identify the architectural features of Radeon GPUs that benefit AI workloads, including the CDNA (Compute DNA) architecture optimized for data center computing, high-bandwidth memory (HBM) for fast data access, and specialized matrix multiplication accelerators. Students should explain the performance characteristics of Radeon GPUs for both AI training (creating models) and inference (using models), including their computational throughput (measured in TFLOPS), memory bandwidth, and energy efficiency. They should understand how Radeon GPUs compare to competing solutions like NVIDIA's RTX series, with Radeon typically offering strong performance in specific AI workloads, particularly those that benefit from open-source software support. Students should recognize that while NVIDIA currently leads in the AI software ecosystem, AMD has been making significant strides with ROCm and improved framework support. Examples should include benchmark comparisons between Radeon and competing GPUs for specific AI tasks like image classification or natural language processing.

**7.7.12 Understand AMD EPYC processors for data center AI**
- Define AMD EPYC as a line of server processors optimized for data center workloads
- Identify the features of EPYC processors that benefit AI applications
- Explain how EPYC processors complement GPUs in AI systems
- Understand the role of EPYC processors in large-scale AI deployments

**Guidance:** Students should understand AMD EPYC as a line of server processors designed for high-performance computing and data center workloads, including AI applications. They should identify the features of EPYC processors that benefit AI applications, including high core counts (up to 96 cores in current generations), support for large amounts of memory (up to 12TB per socket), high memory bandwidth, and PCIe 5.0 connectivity for fast communication with GPUs and other accelerators. Students should explain how EPYC processors complement GPUs in AI systems by handling data preprocessing, task scheduling, and parts of AI algorithms that don't parallelize well on GPUs, while also providing fast access to memory and storage. They should understand the role of EPYC processors in large-scale AI deployments, where they serve as the foundation for systems that may include multiple GPUs per processor and scale to hundreds or thousands of processors in supercomputing environments. Examples should include how EPYC processors are used in AI supercomputers and cloud computing platforms to provide the CPU infrastructure needed for large-scale AI training and inference.

**7.7.13 Explain AMD's adaptive SoCs and FPGAs for AI**
- Define AMD's adaptive SoCs (System on Chip) and FPGAs (Field-Programmable Gate Arrays)
- Identify the features of adaptive computing solutions that benefit AI applications
- Explain how FPGAs can be optimized for specific AI workloads
- Understand the use cases for adaptive computing in AI systems

**Guidance:** Students should understand AMD's adaptive computing solutions, including adaptive SoCs (System on Chip) and FPGAs (Field-Programmable Gate Arrays), as flexible hardware platforms that can be customized for specific AI workloads. They should identify the features of adaptive computing solutions that benefit AI applications, including the ability to implement custom hardware accelerators for specific AI algorithms, reconfigurability (allowing the same hardware to be optimized for different tasks), and low-latency processing. Students should explain how FPGAs can be optimized for specific AI workloads by programming their logic gates to implement specialized circuits that accelerate particular operations, such as convolution layers in neural networks or specific data preprocessing tasks. They should understand the use cases for adaptive computing in AI systems, including edge computing (where power efficiency and customization are critical), data center acceleration (for specific high-value workloads), and rapid prototyping of new AI algorithms. Examples should include how FPGAs are used in financial trading systems for low-latency AI inference or in edge devices for customized AI processing.

**7.7.14 Analyze AMD's software ecosystem for AI development**
- Identify key components of AMD's software ecosystem for AI (ROCm, MIOpen, libraries)
- Explain how AMD's software stack supports AI development
- Understand the relationship between hardware and software in AMD's AI ecosystem
- Compare AMD's software approach with competing platforms

**Guidance:** Students should be able to analyze AMD's software ecosystem for AI development, which is designed to enable developers to leverage AMD hardware for AI applications. They should identify key components of this ecosystem, including ROCm (Radeon Open Compute platform, AMD's alternative to CUDA), MIOpen (optimized library for deep learning), and various mathematical libraries that provide optimized implementations of common AI operations. Students should explain how AMD's software stack supports AI development by providing drivers, runtime libraries, development tools, and framework integrations that allow popular AI frameworks like TensorFlow and PyTorch to run efficiently on AMD hardware. They should understand the relationship between hardware and software in AMD's AI ecosystem, with software specifically designed to extract maximum performance from AMD's GPU and CPU architectures through features like explicit memory management and kernel optimization. Students should compare AMD's software approach with competing platforms, particularly NVIDIA's CUDA ecosystem, recognizing that while NVIDIA currently has broader adoption and more mature tools, AMD's open-source approach offers advantages in customization, transparency, and long-term sustainability. Examples should include developing AI applications using AMD's software stack and comparing the experience with other platforms.

**7.7.15 Compare AMD AI solutions with competing platforms**
- Identify key competitors in the AI hardware market (NVIDIA, Intel, Google, Apple)
- Explain the strengths and weaknesses of AMD's AI solutions relative to competitors
- Understand the market positioning of AMD's AI products
- Apply comparative analysis to select appropriate AI hardware for specific scenarios

**Guidance:** Students should be able to compare AMD's AI solutions with those of key competitors in the AI hardware market. They should identify major competitors including NVIDIA (dominant in AI GPUs with CUDA ecosystem), Intel (developing AI accelerators and CPUs with AI features), Google (TPUs for cloud AI), and Apple (Neural Engine in mobile devices). Students should explain the strengths and weaknesses of AMD's AI solutions relative to competitors, with strengths including strong performance-per-watt, open-source approach, and competitive pricing, and weaknesses including less mature software ecosystem and smaller market share in data centers. They should understand the market positioning of AMD's AI products, with Ryzen AI targeting consumer devices, Radeon GPUs targeting gaming and professional visualization, EPYC targeting data centers, and adaptive solutions targeting specialized applications. Students should apply this comparative analysis to select appropriate AI hardware for specific scenarios, considering factors like performance requirements, software compatibility, power constraints, and budget. Examples should include selecting the best hardware solution for specific AI applications like edge computing, cloud training, or real-time inference.

**7.7.16 Explain the architectural advantages of GPUs for AI/ML workloads**
- Define the architectural features that make GPUs well-suited for AI/ML
- Explain the concept of parallel processing in GPU architecture
- Identify specific GPU architectural elements that benefit AI (SIMD units, tensor cores)
- Understand how GPU architecture aligns with the computational patterns of neural networks

**Guidance:** Students should understand the specific architectural features that make GPUs particularly well-suited for AI/ML workloads compared to other processors. They should define these features, including the massively parallel architecture (thousands of simple cores versus a few complex cores in CPUs), high memory bandwidth (enabling fast data transfer), and specialized hardware units for common mathematical operations. Students should explain the concept of parallel processing in GPU architecture, where many processing elements simultaneously perform the same operation on different data points, which aligns perfectly with the matrix multiplications and convolutions that dominate neural network computations. They should identify specific GPU architectural elements that benefit AI, including SIMD (Single Instruction, Multiple Data) units that execute the same instruction on multiple data elements, and tensor cores (specialized units optimized for matrix operations common in deep learning). Students should understand how these architectural features align with the computational patterns of neural networks, which typically involve applying the same operation across large datasets, making GPUs significantly more efficient than CPUs for these workloads. Examples should include comparing how a GPU and CPU would approach the same neural network computation differently.

**7.7.17 Understand parallel processing in GPU computing**
- Define parallel processing as the simultaneous execution of multiple computations
- Explain how GPUs achieve parallelism through their architecture
- Identify different levels of parallelism in GPU computing (thread, block, grid)
- Understand how parallel processing improves performance for AI/ML workloads

**Guidance:** Students should understand parallel processing as a computational approach where multiple calculations are performed simultaneously, rather than sequentially. They should explain how GPUs achieve parallelism through their architectural design, which includes thousands of smaller, simpler processing cores (compared to CPUs with fewer, more complex cores), wide memory buses (enabling high memory bandwidth), and specialized hardware for scheduling and managing parallel tasks. Students should identify the different levels of parallelism in GPU computing, including threads (individual sequences of instructions), blocks (groups of threads that can synchronize), and grids (collections of blocks that constitute a complete kernel execution). They should understand how this hierarchical parallelism maps well to the structure of neural networks, where operations can be performed independently on different data points. Students should understand how parallel processing improves performance for AI/ML workloads by enabling the computation of thousands or millions of operations simultaneously, rather than one at a time, resulting in dramatic speedups for tasks like training neural networks or running inference on large models. Examples should include illustrating how a convolution operation in a neural network can be parallelized across GPU threads.

**7.7.18 Explain how GPUs accelerate neural network training**
- Define neural network training as the process of adjusting model parameters to minimize error
- Explain how GPUs accelerate the forward and backward passes in training
- Identify specific GPU features that benefit training (large memory, high memory bandwidth)
- Understand the impact of GPU acceleration on training time and model complexity

**Guidance:** Students should understand neural network training as the computationally intensive process of adjusting model parameters (weights and biases) through optimization algorithms like gradient descent to minimize the error between predicted and actual outputs. They should explain how GPUs accelerate both the forward pass (computing outputs from inputs) and backward pass (computing gradients and updating parameters) in training by performing the many matrix multiplications and convolutions in parallel across thousands of cores. Students should identify specific GPU features that benefit training, including large memory capacity (to hold model parameters, activations, and gradients), high memory bandwidth (to quickly move data between memory and processors), and specialized math units (to accelerate common operations like matrix multiplication). They should understand the impact of GPU acceleration on training time and model complexity, including how GPUs can reduce training times from weeks or months to days or hours, and how they enable the training of larger, more complex models that would be impractical to train with CPUs alone. Examples should include comparing the training time of a neural network on a CPU versus a GPU, or showing how GPU acceleration has enabled the development of increasingly large and complex models over time.

**7.7.19 Understand how GPUs accelerate AI inference**
- Define AI inference as the process of using trained models to make predictions
- Explain how GPUs accelerate inference for different types of models
- Identify specific techniques for optimizing GPU inference (batching, quantization, pruning)
- Understand the trade-offs between inference speed, accuracy, and resource usage

**Guidance:** Students should understand AI inference as the process of using trained models to make predictions or decisions based on new input data, which is typically less computationally intensive than training but still benefits from acceleration. They should explain how GPUs accelerate inference for different types of models by parallelizing the computations required to transform inputs into outputs, particularly for models that process multiple inputs simultaneously (batching) or have large internal representations. Students should identify specific techniques for optimizing GPU inference, including batching (processing multiple inputs simultaneously to better utilize GPU parallelism), quantization (reducing the precision of numerical representations to decrease memory usage and increase speed), and pruning (removing less important model parameters to reduce computational complexity). They should understand the trade-offs between inference speed, accuracy, and resource usage, recognizing that optimization techniques like quantization and pruning can improve speed and reduce resource requirements but may slightly decrease model accuracy. Examples should include comparing inference speed and accuracy for a model before and after optimization techniques, or showing how different batch sizes affect GPU utilization and inference latency.

**7.7.20 Identify specialized GPU features for AI (tensor cores, etc.)**
- Define specialized GPU features designed specifically for AI workloads
- Identify tensor cores and their function in accelerating matrix operations
- Explain other AI-specific GPU features (mixed precision, sparsity support)
- Understand how these specialized features improve AI performance and efficiency

**Guidance:** Students should be able to identify and explain specialized GPU features that have been specifically designed to accelerate AI workloads. They should define tensor cores as specialized processing units in modern GPUs that are optimized for matrix multiplications and accumulations, which are the fundamental operations in neural networks. Students should explain the function of tensor cores in accelerating matrix operations by performing multiple multiply-accumulate operations in parallel, significantly speeding up the convolutions and fully connected layers that dominate neural network computations. They should explain other AI-specific GPU features, including mixed precision support (the ability to perform calculations with reduced numerical precision like 16-bit or 8-bit formats, which improves speed and reduces memory usage) and sparsity support (hardware optimizations for models with many zero-valued parameters, which can skip unnecessary computations). Students should understand how these specialized features improve AI performance and efficiency by providing dedicated hardware for the most common operations in neural networks, enabling faster training and inference with lower power consumption. Examples should include comparing the performance of a neural network on a GPU with and without tensor cores, or showing the performance benefits of mixed precision training.

**7.7.21 Explain memory bandwidth and its importance for AI**
- Define memory bandwidth as the rate at which data can be read from or written to memory
- Explain why memory bandwidth is critical for AI workloads
- Identify factors that affect memory bandwidth in GPU systems
- Understand the relationship between memory bandwidth and AI performance

**Guidance:** Students should understand memory bandwidth as a measure of how quickly data can be transferred between the memory and the processor, typically expressed in GB/s (gigabytes per second). They should explain why memory bandwidth is particularly critical for AI workloads because neural networks often involve processing large amounts of data, and the computational units can become starved for data if the memory system can't keep up with the processing capabilities. Students should identify factors that affect memory bandwidth in GPU systems, including memory technology (GDDR6, HBM2, etc.), memory bus width (the number of data lines that can transfer data simultaneously), and memory clock speed (how fast data can be transferred on each line). They should understand the relationship between memory bandwidth and AI performance, recognizing that insufficient memory bandwidth can create bottlenecks that prevent even powerful processors from achieving their full potential. Examples should include comparing the performance of the same GPU with different memory configurations, or showing how memory bandwidth requirements scale with model size and batch size.

**7.7.22 Understand GPU programming models for AI (CUDA, ROCm, etc.)**
- Define GPU programming models as software frameworks for leveraging GPU hardware
- Identify major GPU programming models (CUDA, ROCm, OpenCL, SYCL)
- Explain how these programming models enable AI development on GPUs
- Understand the differences between proprietary and open-source programming models

**Guidance:** Students should understand GPU programming models as software frameworks that provide APIs, libraries, and tools for developers to write applications that can leverage GPU hardware acceleration. They should identify major GPU programming models, including CUDA (NVIDIA's proprietary platform for GPU computing), ROCm (AMD's open-source platform for GPU computing), OpenCL (an open standard for heterogeneous computing), and SYCL (a higher-level, cross-platform abstraction for heterogeneous computing). Students should explain how these programming models enable AI development on GPUs by providing abstractions for managing GPU memory, launching parallel computations, and implementing common mathematical operations used in neural networks. They should understand the differences between proprietary and open-source programming models, with proprietary models like CUDA typically offering more mature tools and broader framework support but being limited to specific hardware, while open-source models like ROCm offer greater flexibility, transparency, and potential for community improvement but may have less polished tools and narrower framework support. Examples should include comparing the experience of developing AI applications using CUDA versus ROCm, or explaining how the same neural network might be implemented differently using different programming models.

**7.7.23 Apply GPU computing concepts to different AI domains**
- Identify different AI domains and their computational characteristics
- Explain how GPU computing benefits specific AI domains
- Understand domain-specific GPU optimizations and techniques
- Apply GPU computing knowledge to solve problems in various AI domains

**Guidance:** Students should be able to apply their understanding of GPU computing to different domains within artificial intelligence. They should identify various AI domains and their computational characteristics, including computer vision (processing images and videos), natural language processing (analyzing text and speech), reinforcement learning (training agents through interaction), and generative AI (creating new content). Students should explain how GPU computing benefits specific AI domains, such as how the parallel processing capabilities of GPUs accelerate the convolution operations in computer vision, the matrix multiplications in natural language processing, the simulations in reinforcement learning, and the sampling processes in generative AI. They should understand domain-specific GPU optimizations and techniques, including specialized kernels for convolution operations in computer vision, memory optimizations for large language models, and parallel simulation techniques for reinforcement learning. Students should apply their GPU computing knowledge to solve problems in various AI domains by selecting appropriate GPU hardware, programming models, and optimization techniques based on the specific requirements of each domain. Examples should include designing GPU-accelerated solutions for specific applications in computer vision, natural language processing, or other AI domains.

**7.7.24 Explain hardware bottlenecks in AI systems**
- Define hardware bottlenecks as components that limit overall system performance
- Identify common bottlenecks in AI systems (compute, memory, storage, interconnect)
- Explain how to identify and diagnose bottlenecks in AI systems
- Understand the impact of bottlenecks on AI performance and efficiency

**Guidance:** Students should understand hardware bottlenecks as components in a system that limit overall performance by creating constraints that other components cannot overcome. They should identify common bottlenecks in AI systems, including compute bottlenecks (insufficient processing power for the computational requirements), memory bottlenecks (insufficient memory capacity or bandwidth to hold and access data), storage bottlenecks (inability to supply data to the system quickly enough), and interconnect bottlenecks (limitations in data transfer between components). Students should explain how to identify and diagnose bottlenecks in AI systems using monitoring tools, performance profiling, and analysis of resource utilization patterns. They should understand the impact of bottlenecks on AI performance and efficiency, including how they can prevent expensive components from being fully utilized, increase training or inference times, and reduce the overall cost-effectiveness of AI systems. Examples should include analyzing a hypothetical AI system to identify the limiting component and suggesting upgrades or optimizations to alleviate the bottleneck.

**7.7.25 Understand balancing components for optimal AI performance**
- Explain the concept of balanced system design for AI workloads
- Identify principles for balancing CPU, GPU, memory, and storage in AI systems
- Understand how to match component capabilities to avoid bottlenecks
- Apply balancing principles to design efficient AI systems

**Guidance:** Students should understand the concept of balanced system design for AI workloads, which involves selecting and configuring components so that no single component creates a significant bottleneck that limits overall performance. They should identify principles for balancing CPU, GPU, memory, and storage in AI systems, including ensuring that the CPU can supply data to the GPU quickly enough, that memory capacity is sufficient to hold working datasets and model parameters, and that storage can supply data to memory at the required rate. Students should understand how to match component capabilities to avoid bottlenecks by analyzing the requirements of specific AI workloads and selecting components that can handle those requirements without significant imbalances. They should apply these balancing principles to design efficient AI systems by creating configurations where each component can handle its share of the workload without overwhelming or being overwhelmed by other components. Examples should include designing balanced systems for different AI scenarios, such as a small-scale development system, a medium-scale training system, or a large-scale inference deployment.

**7.7.26 Explain cooling solutions for high-performance AI systems**
- Identify the cooling challenges in high-performance AI systems
- Explain different cooling solutions for AI hardware (air cooling, liquid cooling, immersion cooling)
- Understand the relationship between cooling, performance, and reliability
- Apply cooling knowledge to design AI systems with appropriate thermal management

**Guidance:** Students should understand the significant cooling challenges in high-performance AI systems, which generate substantial heat due to the intensive computational workloads and high power consumption of components like GPUs and specialized accelerators. They should explain different cooling solutions for AI hardware, including air cooling (using fans and heat sinks to dissipate heat), liquid cooling (circulating liquid to absorb and transfer heat away from components), and immersion cooling (submerging components in thermally conductive but electrically insulating liquid). Students should understand the relationship between cooling, performance, and reliability in AI systems, including how inadequate cooling can lead to thermal throttling (reducing performance to prevent overheating), decreased component lifespan, and system instability. They should apply this cooling knowledge to design AI systems with appropriate thermal management by selecting cooling solutions that match the thermal output of the components, considering factors like noise levels, maintenance requirements, and environmental conditions. Examples should include selecting appropriate cooling solutions for different AI deployments, from a single workstation to a large data center.

**7.7.27 Understand power requirements and efficiency in AI systems**
- Explain the power requirements of AI hardware components
- Identify factors affecting power consumption in AI systems (workload intensity, hardware efficiency)
- Understand the concept of performance-per-watt as a metric for AI systems
- Apply power efficiency knowledge to design sustainable AI systems

**Guidance:** Students should understand the substantial power requirements of AI hardware components, particularly high-performance GPUs and accelerators that can consume hundreds of watts each. They should identify factors affecting power consumption in AI systems, including workload intensity (how hard the hardware is working), hardware efficiency (how effectively the hardware converts electricity to computation), and operational patterns (training versus inference, batch size, etc.). Students should understand the concept of performance-per-watt as a critical metric for AI systems, measuring how much computational performance can be achieved per watt of power consumed, which is particularly important for large-scale deployments where electricity costs can be significant. They should apply this power efficiency knowledge to design sustainable AI systems by selecting components with good performance-per-watt ratios, implementing power management techniques, and optimizing workloads to minimize energy consumption while maintaining required performance levels. Examples should include comparing the power efficiency of different AI hardware configurations or calculating the energy costs of training large models over extended periods.

**7.7.28 Explain scaling approaches for AI hardware (single system to clusters)**
- Define different scaling approaches for AI systems (vertical scaling, horizontal scaling)
- Explain how to scale AI systems from single workstations to large clusters
- Identify challenges in scaling AI systems (communication overhead, synchronization)
- Understand distributed computing frameworks for large-scale AI

**Guidance:** Students should understand different approaches to scaling AI hardware to meet increasing computational demands. They should define vertical scaling (increasing the capabilities of a single system by adding more powerful components) and horizontal scaling (increasing the number of systems working together) as the two primary approaches to scaling AI systems. Students should explain how to scale AI systems from single workstations (suitable for development and small models) to multi-GPU systems (for medium-scale training) to large clusters (for training very large models or serving many inference requests). They should identify challenges in scaling AI systems, including communication overhead (time spent transferring data between systems), synchronization (ensuring all systems are working on the right parts of a problem), and fault tolerance (handling failures in large systems). Students should understand distributed computing frameworks for large-scale AI, such as Horovod, PyTorch Distributed, and TensorFlow Distributed, which provide abstractions and tools for training models across multiple systems. Examples should include designing scaling strategies for different AI scenarios, from a research lab to a large cloud deployment.

**7.7.29 Understand hardware virtualization for AI workloads**
- Define hardware virtualization in the context of AI systems
- Explain how virtualization technologies enable sharing of AI hardware resources
- Identify benefits and challenges of virtualization for AI workloads
- Understand containerization approaches for AI deployment (Docker, Kubernetes)

**Guidance:** Students should understand hardware virtualization as a technology that allows multiple virtual machines or containers to share physical hardware resources, which is particularly important for efficiently utilizing expensive AI hardware like GPUs. They should explain how virtualization technologies enable sharing of AI hardware resources by creating logical partitions of physical devices that can be allocated to different users or tasks, allowing multiple workloads to run concurrently on the same hardware. Students should identify the benefits of virtualization for AI workloads, including improved resource utilization, isolation between users and tasks, and easier management, as well as challenges like potential performance overhead and complexity in configuration. They should understand containerization approaches for AI deployment, particularly Docker (which packages applications and their dependencies into lightweight, portable containers) and Kubernetes (which orchestrates container deployment and scaling), which have become popular for deploying AI applications in cloud and data center environments. Examples should include setting up virtualized GPU environments for multiple users or deploying containerized AI models using Kubernetes.

**7.7.30 Explain performance metrics and benchmarking for AI systems**
- Define key performance metrics for AI hardware (FLOPS, TOPS, memory bandwidth, latency)
- Explain benchmarking approaches for AI systems (standard benchmarks, custom benchmarks)
- Understand how to interpret benchmark results for AI hardware selection
- Apply benchmarking knowledge to evaluate and compare AI systems

**Guidance:** Students should understand the key performance metrics used to evaluate and compare AI hardware. They should define metrics including FLOPS (Floating Point Operations Per Second, measuring raw computational performance), TOPS (Tera Operations Per Second, often used for integer operations in inference), memory bandwidth (data transfer rate), and latency (response time for operations). Students should explain benchmarking approaches for AI systems, including standard benchmarks like MLPerf (which measures performance across a suite of AI tasks) and custom benchmarks (which simulate specific workloads relevant to particular applications). They should understand how to interpret benchmark results for AI hardware selection by considering not just peak performance numbers but also real-world performance on relevant tasks, efficiency metrics like performance-per-watt, and total cost of ownership. Students should apply this benchmarking knowledge to evaluate and compare AI systems by running appropriate benchmarks, analyzing the results, and making informed decisions about hardware selection based on specific requirements and constraints. Examples should include comparing different AI systems using standard benchmarks or designing custom benchmarks for specific applications.

# Topic 8: Hardware Maintenance and Troubleshooting

Students will be assessed on their ability to:

**8.8.1 Define hardware maintenance and its importance**
- Define hardware maintenance as the process of keeping computer equipment in good working condition
- Explain the importance of regular hardware maintenance for system reliability and longevity
- Identify the goals of hardware maintenance (prevention, optimization, lifespan extension)
- Understand the relationship between maintenance and total cost of ownership

**Guidance:** Students should understand hardware maintenance as a systematic process of inspecting, cleaning, repairing, and replacing computer components to ensure optimal performance and prevent failures. They should explain the importance of regular hardware maintenance in preventing unexpected downtime, extending equipment lifespan, maintaining system performance, and protecting data integrity. Students should identify the primary goals of hardware maintenance: prevention (avoiding problems before they occur), optimization (ensuring components operate at peak efficiency), and lifespan extension (maximizing the useful life of hardware investments). They should understand the relationship between maintenance and total cost of ownership, recognizing that while maintenance requires time and resources, it ultimately reduces costs by preventing expensive failures, extending replacement cycles, and maintaining productivity. Examples should include comparing the lifecycle costs of well-maintained versus neglected computer systems.

**8.8.2 Explain preventive maintenance procedures for computer systems**
- Identify routine preventive maintenance tasks for computer systems
- Explain the proper procedures for cleaning computer components
- Understand the importance of maintaining proper environmental conditions
- Apply preventive maintenance knowledge to create maintenance schedules

**Guidance:** Students should be able to identify and explain routine preventive maintenance tasks that help keep computer systems functioning properly. They should describe procedures for cleaning various components, including compressed air for removing dust from internal components, isopropyl alcohol for cleaning contacts, microfiber cloths for screens and surfaces, and specialized cleaning tools for keyboards and other peripherals. Students should understand the importance of maintaining proper environmental conditions, including temperature control (keeping systems within recommended operating temperatures), humidity management (preventing moisture-related damage), air quality (reducing dust and particulate matter), and electrical conditions (using surge protection and stable power). They should apply this knowledge to create comprehensive maintenance schedules that specify daily, weekly, monthly, and annual maintenance tasks appropriate for different types of systems and environments. Examples should include developing a maintenance plan for a home computer, a business workstation, or a server environment.

**8.8.3 Explain the tools and equipment needed for hardware maintenance**
- Identify essential tools for basic hardware maintenance
- Explain the purpose of specialized diagnostic and testing equipment
- Understand safety equipment and procedures for hardware maintenance
- Apply knowledge of tools to select appropriate equipment for specific maintenance tasks

**Guidance:** Students should be able to identify and explain the various tools and equipment needed for effective hardware maintenance. They should recognize essential hand tools including screwdrivers (Phillips and flathead in various sizes), nut drivers, Torx drivers, tweezers, and pliers for basic disassembly and assembly. Students should explain the purpose of specialized diagnostic and testing equipment, including multimeters (for testing electrical components), POST (Power-On Self-Test) cards (for diagnosing boot problems), loopback plugs (for testing ports), and software diagnostic tools. They should understand safety equipment and procedures, including anti-static wrist straps (to prevent electrostatic discharge), anti-static mats, proper grounding techniques, and personal protective equipment like safety glasses and gloves. Students should apply this knowledge to select appropriate tools and equipment for specific maintenance tasks, such as building a basic toolkit for home computer maintenance or specifying tools for a professional repair environment. Examples should include demonstrating the proper use of anti-static protection when working with internal components.

**8.8.4 Explain proper cleaning techniques for computer components**
- Identify appropriate cleaning methods for different computer components
- Explain the procedures for safely cleaning internal and external components
- Understand the importance of using proper cleaning materials and solutions
- Apply cleaning techniques to maintain various computer systems

**Guidance:** Students should be able to identify and explain appropriate cleaning methods for different computer components, recognizing that each part requires specific care. They should explain procedures for safely cleaning external components like monitors (using microfiber cloths and screen-specific cleaning solutions), keyboards (using compressed air, brushes, and cleaning putty), and mice (wiping with appropriate cleaning solutions). Students should also explain procedures for cleaning internal components, including using compressed air to remove dust from fans, heat sinks, and circuit boards, isopropyl alcohol for cleaning contacts, and specialized tools for delicate components. They should understand the importance of using proper cleaning materials and solutions, avoiding household cleaners that can damage electronics, using lint-free cloths to prevent fiber residue, and ensuring cleaning solutions are applied to cloths rather than directly to components. Students should apply these cleaning techniques to maintain various computer systems, from personal laptops to business workstations to servers. Examples should include demonstrating the proper cleaning of a dusty desktop computer interior or a keyboard with spilled liquid.

**8.8.5 Explain the importance of firmware and driver updates**
- Define firmware and drivers in the context of computer systems
- Explain how firmware and driver updates improve system performance and security
- Identify procedures for safely updating firmware and drivers
- Understand the risks associated with firmware updates and how to mitigate them

**Guidance:** Students should understand firmware as low-level software embedded in hardware components that controls their basic functions, and drivers as software that allows the operating system to communicate with hardware devices. They should explain how firmware and driver updates improve system performance by fixing bugs, enhancing compatibility, adding features, and optimizing performance, while also improving security by patching vulnerabilities that could be exploited by malicious actors. Students should identify procedures for safely updating firmware and drivers, including identifying current versions, downloading updates from official sources, creating system backups, following manufacturer instructions precisely, and verifying successful updates. They should understand the risks associated with firmware updates, including the potential for "bricking" devices (rendering them inoperable) if updates fail or are interrupted, and how to mitigate these risks through stable power sources, following procedures carefully, and having recovery options available. Examples should include updating a motherboard BIOS or a graphics card driver, with emphasis on safety precautions.

**8.8.6 Explain common hardware failure modes and symptoms**
- Identify typical failure modes for major computer components
- Explain the symptoms associated with different hardware failures
- Understand the relationship between failure modes and their causes
- Apply knowledge of failure symptoms to diagnose potential hardware issues

**Guidance:** Students should be able to identify and explain typical failure modes for major computer components and their associated symptoms. They should describe CPU failure modes including overheating (symptoms: system shutdowns, thermal throttling, performance degradation) and physical damage (symptoms: system failure to boot, random crashes). Students should explain RAM failure symptoms including system instability, blue screens, and failure to boot, often accompanied by specific error codes or beep patterns. They should identify storage device failure modes including mechanical failure in HDDs (symptoms: clicking noises, slow performance, file corruption) and wear-leveling issues in SSDs (symptoms: performance degradation, read/write errors). Students should understand power supply failure symptoms including random shutdowns, failure to power on, and unusual noises or smells. They should explain graphics card failure symptoms including visual artifacts, display issues, and system crashes during graphics-intensive tasks. Students should apply this knowledge to diagnose potential hardware issues by matching observed symptoms to likely failure modes. Examples should include troubleshooting a system that won't boot or one that crashes under load.

**8.8.7 Explain systematic approaches to hardware troubleshooting**
- Define a systematic approach to hardware troubleshooting
- Explain the steps in a structured troubleshooting methodology
- Understand the importance of documentation in troubleshooting processes
- Apply systematic troubleshooting to diagnose hardware problems

**Guidance:** Students should understand a systematic approach to hardware troubleshooting as a structured methodology for identifying, diagnosing, and resolving hardware issues efficiently and effectively. They should explain the steps in a structured troubleshooting methodology, including problem identification (clearly defining the issue), information gathering (collecting relevant data about the problem), hypothesis development (forming theories about potential causes), testing (verifying or refuting hypotheses), solution implementation (applying fixes), and verification (confirming the problem is resolved). Students should understand the importance of documentation in troubleshooting processes, including recording symptoms, steps taken, test results, and solutions implemented, which creates a knowledge base for future issues and helps with communication with other technicians or support services. They should apply this systematic troubleshooting approach to diagnose various hardware problems, following each step methodically rather than randomly trying solutions. Examples should include working through a complete troubleshooting process for a computer that won't power on or one that displays error messages during startup.

**8.8.8 Explain diagnostic tools and techniques for hardware troubleshooting**
- Identify software diagnostic tools for hardware testing
- Explain hardware diagnostic tools and their applications
- Understand how to interpret diagnostic results
- Apply diagnostic tools to troubleshoot specific hardware issues

**Guidance:** Students should be able to identify and explain various diagnostic tools and techniques used in hardware troubleshooting. They should recognize software diagnostic tools including built-in utilities like Windows Memory Diagnostic, macOS Hardware Test, and Linux diagnostic commands, as well as third-party applications like MemTest86 for RAM testing, CrystalDiskInfo for storage health monitoring, and GPU-Z for graphics card information. Students should explain hardware diagnostic tools including POST (Power-On Self-Test) cards that display error codes during boot, multimeters for testing electrical components, loopback plugs for testing ports, and specialized testers for RAM, storage devices, and power supplies. They should understand how to interpret diagnostic results, including error codes, test reports, and performance metrics, to identify specific hardware issues. Students should apply these diagnostic tools to troubleshoot specific hardware issues, selecting appropriate tools based on symptoms and systematically testing components to isolate problems. Examples should include using MemTest86 to diagnose RAM issues or a POST card to troubleshoot a system that won't boot.

**8.8.9 Explain troubleshooting procedures for specific components**
- Identify troubleshooting procedures for CPUs and cooling systems
- Explain troubleshooting procedures for memory modules
- Explain troubleshooting procedures for storage devices
- Explain troubleshooting procedures for graphics cards
- Explain troubleshooting procedures for power supplies
- Apply component-specific troubleshooting to resolve hardware issues

**Guidance:** Students should be able to explain specific troubleshooting procedures for major computer components. For CPUs and cooling systems, they should identify procedures including checking for proper seating, monitoring temperatures, testing cooling fans, and verifying thermal paste application. For memory modules, they should explain procedures including testing modules individually, reseating modules, checking compatibility, and cleaning contacts. For storage devices, they should describe procedures including checking connections, running diagnostic software, listening for unusual noises (HDDs), and checking for firmware updates. For graphics cards, they should explain procedures including reseating the card, checking power connections, updating drivers, and testing with a known-good monitor. For power supplies, they should describe procedures including testing with a multimeter, checking connections, testing with a power supply tester, and verifying adequate wattage for the system. Students should apply these component-specific troubleshooting procedures to systematically diagnose and resolve hardware issues, following the appropriate steps for each component type. Examples should include troubleshooting an overheating CPU, a failing hard drive, or a graphics card with display issues.

**8.8.10 Explain safety procedures for hardware maintenance and troubleshooting**
- Identify electrical safety precautions when working with computer hardware
- Explain proper handling procedures for sensitive components
- Understand static electricity protection methods
- Apply safety procedures when performing hardware maintenance and troubleshooting

**Guidance:** Students should understand and explain critical safety procedures for hardware maintenance and troubleshooting. They should identify electrical safety precautions including disconnecting power before working on systems, using a multimeter to verify power is off, unplugging rather than just shutting down, and using one hand when working with live circuits to prevent electrical current across the chest. Students should explain proper handling procedures for sensitive components, including holding components by the edges rather than touching contacts or circuitry, using appropriate tools to avoid damage, and following manufacturer guidelines for handling specific components. They should understand static electricity protection methods, including using anti-static wrist straps properly connected to ground, working on anti-static mats, avoiding static-generating materials like certain fabrics, and storing components in anti-static bags when not installed. Students should apply these safety procedures consistently when performing hardware maintenance and troubleshooting, recognizing that safety should always be the priority over convenience or speed. Examples should include demonstrating proper anti-static protection when installing RAM or safely opening a power supply for testing.

**8.8.11 Explain hardware upgrade processes and considerations**
- Identify factors to consider before upgrading hardware components
- Explain the process of safely upgrading major components
- Understand compatibility considerations for hardware upgrades
- Apply upgrade knowledge to improve system performance and capabilities

**Guidance:** Students should be able to identify and explain the factors to consider before upgrading hardware components, including performance requirements, budget constraints, compatibility limitations, and potential bottlenecks. They should explain the process of safely upgrading major components, including researching compatible components, preparing the system (backing up data, gathering tools), following proper installation procedures, and testing the upgraded system. Students should understand compatibility considerations for hardware upgrades, including physical compatibility (will the component fit in the case?), electrical compatibility (does the power supply have sufficient capacity and connectors?), interface compatibility (does the motherboard support the component?), and software compatibility (are appropriate drivers available?). They should apply this upgrade knowledge to improve system performance and capabilities by selecting appropriate upgrades that provide meaningful benefits while avoiding unnecessary expenses. Examples should include upgrading RAM to improve system responsiveness, adding an SSD to speed up boot and load times, or replacing a graphics card to enhance gaming or content creation performance.

**8.8.12 Explain compatibility considerations for hardware upgrades**
- Identify physical compatibility factors in hardware upgrades
- Explain electrical compatibility considerations
- Understand interface and protocol compatibility
- Apply compatibility knowledge to select appropriate upgrade components

**Guidance:** Students should understand the various compatibility factors that must be considered when upgrading hardware components. They should identify physical compatibility factors including component dimensions (will the component fit in the available space?), mounting points (does the component have the necessary mounting holes or connectors?), and clearance issues (will the component interfere with other parts?). Students should explain electrical compatibility considerations including power requirements (does the component require more power than the system can supply?), voltage requirements (does the component use the appropriate voltage?), and connector types (does the system have the necessary power connectors?). They should understand interface and protocol compatibility, including socket types for CPUs, memory types and speeds for RAM, expansion slot types for add-on cards, and interface standards for storage devices. Students should apply this compatibility knowledge to select appropriate upgrade components by researching specifications, checking manufacturer documentation, and using compatibility tools when available. Examples should include selecting a compatible graphics card for a specific motherboard and power supply, choosing appropriate RAM for a given system, or determining if a new CPU will work with an existing motherboard.

**8.8.13 Explain performance optimization through hardware upgrades**
- Identify performance bottlenecks in computer systems
- Explain how specific hardware upgrades can address performance limitations
- Understand the concept of balanced system design
- Apply performance optimization knowledge to plan effective upgrade strategies

**Guidance:** Students should be able to identify performance bottlenecks in computer systems and explain how hardware upgrades can address these limitations. They should explain how to identify bottlenecks by monitoring system performance, analyzing resource utilization, and comparing component capabilities with application requirements. Students should explain how specific hardware upgrades can address performance limitations, such as adding RAM to eliminate disk swapping, upgrading to an SSD to reduce storage latency, installing a more powerful GPU to improve graphics performance, or upgrading the CPU to enhance overall system responsiveness. They should understand the concept of balanced system design, where components are selected to work together harmoniously without creating significant bottlenecks, recognizing that upgrading one component without addressing limitations in others may provide limited benefits. Students should apply this performance optimization knowledge to plan effective upgrade strategies that provide the most significant performance improvements for the investment. Examples should include analyzing a system to identify the most cost-effective upgrades for gaming, content creation, or general productivity tasks.

**8.8.14 Explain documentation and record-keeping for hardware maintenance**
- Identify important information to document in hardware maintenance
- Explain the benefits of maintaining detailed hardware records
- Understand best practices for hardware documentation
- Apply documentation practices to track system changes and maintenance history

**Guidance:** Students should understand the importance of documentation and record-keeping in hardware maintenance. They should identify important information to document, including system specifications (make, model, serial numbers), component details (manufacturer, model, specifications), maintenance activities (dates, procedures performed, results), upgrades (components added or replaced, compatibility notes), and issues encountered (symptoms, diagnosis, solutions). Students should explain the benefits of maintaining detailed hardware records, including faster troubleshooting (historical data provides context), better planning (knowing when components were installed helps predict lifespan), warranty management (documentation supports warranty claims), and inventory management (tracking assets for organizations). They should understand best practices for hardware documentation, including using standardized forms or digital systems, recording information immediately after performing work, maintaining secure backups of documentation, and ensuring documentation is accessible to authorized personnel. Students should apply these documentation practices to track system changes and maintenance history for personal computers or organizational assets. Examples should include creating a maintenance log for a computer system or developing a documentation system for a small business IT environment.

**8.8.15 Explain professional resources and support for hardware maintenance**
- Identify professional resources for hardware maintenance information
- Explain the role of manufacturer documentation and support
- Understand when to seek professional assistance for hardware issues
- Apply knowledge of professional resources to resolve complex hardware problems

**Guidance:** Students should be able to identify and explain various professional resources available for hardware maintenance and troubleshooting. They should recognize manufacturer resources including documentation (user manuals, service guides, technical specifications), support websites (knowledge bases, driver downloads, firmware updates), and direct support services (phone support, chat support, repair services). Students should explain the role of manufacturer documentation and support in providing accurate, model-specific information and authorized repair procedures. They should understand when to seek professional assistance for hardware issues, including situations involving complex problems beyond their skill level, safety concerns, warranty considerations, and time constraints that make DIY repair impractical. Students should apply knowledge of professional resources to resolve complex hardware problems by effectively researching issues, following manufacturer procedures, and knowing when to escalate problems to qualified technicians. Examples should include using manufacturer documentation to troubleshoot a specific motherboard issue or determining when a storage device problem requires professional data recovery services.
